{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"/ocean/projects/asc170022p/shg121/PhD/Project_Pruning\"))\n",
    "import h5py\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import utils\n",
    "from model_factory.models import Classifier\n",
    "from model_factory.model_meta import Model_Meta\n",
    "from dataset.dataset_utils import get_dataset_with_attributes, get_transforms\n",
    "from dataset.dataset_attributes_mnist import Dataset_attributes_mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = utils.get_device()\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "root = \"/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/data/MNIST_EVEN_ODD\"\n",
    "model_arch = \"Resnet_18\"\n",
    "dataset_name = \"mnist\"\n",
    "pretrained = False\n",
    "transfer_learning = False\n",
    "chk_pt_path = \"seq_epoch_20.pth.tar\"\n",
    "num_classes = 1\n",
    "logs =\"/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/output\"\n",
    "\n",
    "bb_layers = [\"layer3\", \"layer4\"]\n",
    "size = 224\n",
    "batch_size = 1\n",
    "num_workers = 1\n",
    "concept_names = [\"Zero\", \"One\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\", \"Nine\"]\n",
    "# 0-Even 1-Odd\n",
    "class_list = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_path = os.path.join(logs, \"activations\", \"BB\", model_arch, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output prediction directory is created successfully at:\n",
      "/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/output/activations/BB/Resnet_18/mnist\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.makedirs(activations_path, exist_ok=True)\n",
    "    print(\"output prediction directory is created successfully at:\")\n",
    "    print(activations_path)\n",
    "except OSError as error:\n",
    "    print(f\"output prediction directory {activations_path} can not be created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the [train] dataset: 48000\n",
      "\n",
      "Time to load the dataset from disk: 19.835436820983887 secs\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "transform = get_transforms(size=size)\n",
    "train_set, train_attributes = get_dataset_with_attributes(\n",
    "        data_root=\"/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/data/MNIST_EVEN_ODD\",\n",
    "        json_root=\"/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/scripts_data\",\n",
    "        dataset_name=\"mnist\",\n",
    "        mode=\"train\",\n",
    "        attribute_file=\"attributes.npy\"\n",
    "    )\n",
    "\n",
    "train_dataset = Dataset_attributes_mnist(train_set, train_attributes, transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "done = time.time()\n",
    "elapsed = done - start\n",
    "print(\"Time to load the dataset from disk: \" + str(elapsed) + \" secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the [val] dataset: 12000\n",
      "\n",
      "Time to load the dataset from disk: 5.1964943408966064 secs\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "val_set, val_attributes = get_dataset_with_attributes(\n",
    "        data_root=\"/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/data/MNIST_EVEN_ODD\",\n",
    "        json_root=\"/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/scripts_data\",\n",
    "        dataset_name=\"mnist\",\n",
    "        mode=\"val\",\n",
    "        attribute_file=\"attributes.npy\"\n",
    "    )\n",
    "\n",
    "val_dataset = Dataset_attributes_mnist(val_set, val_attributes, transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "done = time.time()\n",
    "elapsed = done - start\n",
    "print(\"Time to load the dataset from disk: \" + str(elapsed) + \" secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the [test] dataset: 60000\n",
      "\n",
      "Time to load the dataset from disk: 25.02712631225586 secs\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "test_set, test_attributes = get_dataset_with_attributes(\n",
    "        data_root=\"/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/data/MNIST_EVEN_ODD\",\n",
    "        json_root=\"/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/scripts_data\",\n",
    "        dataset_name=\"mnist\",\n",
    "        mode=\"test\",\n",
    "        attribute_file=\"attributes.npy\"\n",
    "    )\n",
    "\n",
    "test_dataset = Dataset_attributes_mnist(test_set, test_attributes, transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "done = time.time()\n",
    "elapsed = done - start\n",
    "print(\"Time to load the dataset from disk: \" + str(elapsed) + \" secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader.dataset), len(val_dataloader.dataset), len(test_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and load hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(model_arch, num_classes, pretrained, transfer_learning).to(device)\n",
    "checkpoint_path = os.path.join(logs, \"chk_pt\", \"BB\", model_arch, dataset_name, chk_pt_path)\n",
    "model_chk_pt = torch.load(checkpoint_path)\n",
    "model.load_state_dict(model_chk_pt)\n",
    "\n",
    "model.eval()\n",
    "model_meta = Model_Meta(model, bb_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create activations and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_activation_DB(dataloader):\n",
    "    attr_GT = torch.FloatTensor()\n",
    "    activations = {}\n",
    "    for l in bb_layers:\n",
    "        activations[l] = []\n",
    "\n",
    "    with tqdm(total=len(dataloader)) as t:\n",
    "        for batch_id, (\n",
    "                image, attribute\n",
    "        ) in enumerate(dataloader):\n",
    "            image = image.to(device)\n",
    "            _ = model(image).cpu().detach()\n",
    "            for l in bb_layers:\n",
    "                z = model_meta.model_activations_store[l].cpu().detach().numpy()\n",
    "                activations[l].append(z)\n",
    "            t.set_postfix(batch_id='{0}'.format(batch_id))\n",
    "            attr_GT = torch.cat((attr_GT, attribute), dim=0)\n",
    "            t.update()\n",
    "            \n",
    "    for l in bb_layers:\n",
    "        activations[l] = np.concatenate(activations[l], axis=0)\n",
    "        \n",
    "    return activations, attr_GT.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48000/48000 [05:15<00:00, 152.34it/s, batch_id=47999]\n"
     ]
    }
   ],
   "source": [
    "train_activations, train_np_attr_GT = create_activation_DB(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12000/12000 [01:11<00:00, 167.50it/s, batch_id=11999]\n"
     ]
    }
   ],
   "source": [
    "val_activations, val_np_attr_GT = create_activation_DB(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [06:53<00:00, 144.97it/s, batch_id=59999]\n"
     ]
    }
   ],
   "source": [
    "test_activations, test_np_attr_GT = create_activation_DB(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48000, 10), (12000, 10), (60000, 10))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_np_attr_GT.shape, val_np_attr_GT.shape, test_np_attr_GT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer3\n",
      "Train: (48000, 256, 14, 14)\n",
      "Val: (12000, 256, 14, 14)\n",
      "Test: (60000, 256, 14, 14)\n",
      "-----\n",
      "layer4\n",
      "Train: (48000, 512, 7, 7)\n",
      "Val: (12000, 512, 7, 7)\n",
      "Test: (60000, 512, 7, 7)\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for layer in bb_layers:\n",
    "    print(f\"{layer}\")\n",
    "    print(f\"Train: {train_activations[layer].shape}\")\n",
    "    print(f\"Val: {val_activations[layer].shape}\")\n",
    "    print(f\"Test: {test_activations[layer].shape}\")\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(os.path.join(activations_path, \"train_activations.h5\"), 'w') as f:\n",
    "    for l in bb_layers:\n",
    "        f.create_dataset(l, data=train_activations[l])\n",
    "        \n",
    "with h5py.File(os.path.join(activations_path, \"val_activations.h5\"), 'w') as f:\n",
    "    for l in bb_layers:\n",
    "        f.create_dataset(l, data=val_activations[l])\n",
    "        \n",
    "with h5py.File(os.path.join(activations_path, \"test_activations.h5\"), 'w') as f:\n",
    "    for l in bb_layers:\n",
    "        f.create_dataset(l, data=test_activations[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(activations_path, \"train_np_attr_GT.npy\"), train_np_attr_GT)\n",
    "np.save(os.path.join(activations_path, \"val_np_attr_GT.npy\"), val_np_attr_GT)\n",
    "np.save(os.path.join(activations_path, \"test_np_attr_GT.npy\"), test_np_attr_GT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_activations(path):\n",
    "    print(path)\n",
    "    activations = {}\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        for k, v in f.items():\n",
    "            activations[k] = np.array(v)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/output/activations/BB/Resnet_18/mnist/train_activations.h5\n"
     ]
    }
   ],
   "source": [
    "train_activations = load_activations(os.path.join(activations_path, \"train_activations.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/output/activations/BB/Resnet_18/mnist/val_activations.h5\n"
     ]
    }
   ],
   "source": [
    "val_activations = load_activations(os.path.join(activations_path, \"val_activations.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/output/activations/BB/Resnet_18/mnist/test_activations.h5\n"
     ]
    }
   ],
   "source": [
    "test_activations = load_activations(os.path.join(activations_path, \"test_activations.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_np_attr_GT = np.load(os.path.join(activations_path, \"train_np_attr_GT.npy\"))\n",
    "val_np_attr_GT = np.load(os.path.join(activations_path, \"val_np_attr_GT.npy\"))\n",
    "test_np_attr_GT = np.load(os.path.join(activations_path, \"test_np_attr_GT.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer3\n",
      "Train: (48000, 256, 14, 14)\n",
      "Val: (12000, 256, 14, 14)\n",
      "Test: (60000, 256, 14, 14)\n",
      "-----\n",
      "layer4\n",
      "Train: (48000, 512, 7, 7)\n",
      "Val: (12000, 512, 7, 7)\n",
      "Test: (60000, 512, 7, 7)\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "bb_layers = [\"layer3\", \"layer4\"]\n",
    "for layer in bb_layers:\n",
    "    print(f\"{layer}\")\n",
    "    print(f\"Train: {train_activations[layer].shape}\")\n",
    "    print(f\"Val: {val_activations[layer].shape}\")\n",
    "    print(f\"Test: {test_activations[layer].shape}\")\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48000, 10), (12000, 10), (60000, 10))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_np_attr_GT.shape, val_np_attr_GT.shape, test_np_attr_GT.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer3': 14, 'layer4': 7}\n",
      "layer3\n",
      "Train: (48000, 256)\n",
      "Val: (60000, 256)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1532/1290742443.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train: {flatten_train_activations[layer].shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Val: {flatten_val_activations[layer].shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test: {flatten_test_activations[layer].shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-----\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "def flatten_activations(activations, kernel_size, stride=1):\n",
    "    max_pool = torch.nn.MaxPool2d(kernel_size, stride=1)\n",
    "    torch_activation = torch.from_numpy(activations)\n",
    "    max_pool_activation = max_pool(torch_activation)\n",
    "    flatten_activations = max_pool_activation.view(\n",
    "            max_pool_activation.size()[0], -1\n",
    "        ).numpy()\n",
    "    return flatten_activations\n",
    "\n",
    "\n",
    "flatten_train_activations = {}\n",
    "flatten_val_activations = {}\n",
    "flatten_test_activations = {}\n",
    "\n",
    "kernel_size = {}\n",
    "for layer in bb_layers:\n",
    "    kernel_size[layer] = train_activations[layer].shape[-1]\n",
    "\n",
    "print(kernel_size)\n",
    "for l in bb_layers:\n",
    "    flatten_train_activations[l] = []\n",
    "    flatten_val_activations[l] = []\n",
    "    flatten_test_activations[l] = []\n",
    "    \n",
    "for layer in bb_layers:\n",
    "    print(f\"{layer}\")\n",
    "    flatten_train_activations[layer] = flatten_activations(\n",
    "        train_activations[layer],\n",
    "        kernel_size=kernel_size[layer], \n",
    "        stride=1\n",
    "    )  \n",
    "    flatten_val_activations[layer] = flatten_activations(\n",
    "        val_activations[layer],\n",
    "        kernel_size=kernel_size[layer], \n",
    "        stride=1\n",
    "    )\n",
    "    flatten_test_activations[layer] = flatten_activations(\n",
    "        test_activations[layer],\n",
    "        kernel_size=kernel_size[layer], \n",
    "        stride=1\n",
    "    )      \n",
    "    print(f\"Train: {flatten_train_activations[layer].shape}\")\n",
    "    print(f\"Val: {flatten_val_activations[layer].shape}\")\n",
    "    print(f\"Test: {flatten_test_activations[layer].shape}\")\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_CAVS(x_train, y_train, x_val, y_val):\n",
    "    x_train, y_train = shuffle(x_train, y_train, random_state=0)\n",
    "    print(f\"Train set size, x_train: {x_train.shape} y_train: {y_train.shape}\")\n",
    "    print(f\"Val set size, x_val: {x_val.shape} y_val: {y_val.shape}\")\n",
    "    \n",
    "    start = time.time()\n",
    "    concept_model = LogisticRegression(solver=\"liblinear\")\n",
    "    concept_model.fit(x_train, y_train)\n",
    "    \n",
    "    y_pred = concept_model.predict(x_val)\n",
    "    done = time.time()\n",
    "    elapsed = done - start\n",
    "    print(\"Time to train: \" + str(elapsed) + \" secs\")\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(y_pred=y_pred, y_true=y_val)\n",
    "    auc = metrics.roc_auc_score(y_score=y_pred, y_true=y_val)\n",
    "    precision = metrics.precision_score(y_pred=y_pred, y_true=y_val)\n",
    "    recall = metrics.recall_score(y_pred=y_pred, y_true=y_val)\n",
    "    print(f\"Val set, Accuracy: {accuracy}\")\n",
    "    print(f\"Val set, ROC_AUC: {auc}\")\n",
    "    print(f\"Val set, Precision: {precision}\")\n",
    "    print(f\"Val set, Recall: {recall}\")\n",
    "    print(f\"Coeffs: {len(concept_model.coef_)}\")\n",
    "    if len(concept_model.coef_) == 1:\n",
    "        cav = np.array([-concept_model.coef_[0], concept_model.coef_[0]])\n",
    "        return cav\n",
    "    else:\n",
    "        cav = -np.array(concept_model.coef_)\n",
    "        return cav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CAVs using Resnet18 layers: \")\n",
    "train_cavs = {}\n",
    "for l in bb_layers:\n",
    "    train_cavs[l] = {0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]}\n",
    "    \n",
    "for layer in bb_layers:\n",
    "    print(f\"{layer}\")\n",
    "    print(f\"Train: {flatten_train_activations[layer].shape}\")\n",
    "    print(f\"Val: {flatten_val_activations[layer].shape}\")\n",
    "    print(f\"Test: {flatten_test_activations[layer].shape}\")\n",
    "    for i in range(len(concept_names)):\n",
    "        print(f\"============== >>Logistic regression for attribute: {i} <<================\")\n",
    "        cav = generate_CAVS(\n",
    "                flatten_train_activations[layer], \n",
    "                train_np_attr_GT[:, i],\n",
    "                flatten_val_activations[layer],\n",
    "                val_np_attr_GT[:, i]\n",
    "            )\n",
    "        print(cav)\n",
    "        train_cavs[layer][i] = cav\n",
    "    print(\"------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TCAV score for each concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/data/MNIST_EVEN_ODD\"\n",
    "model_arch = \"Resnet_18\"\n",
    "dataset_name = \"mnist\"\n",
    "pretrained = False\n",
    "transfer_learning = False\n",
    "chk_pt_path = \"seq_epoch_20.pth.tar\"\n",
    "num_classes = 1\n",
    "logs = \"/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/output\"\n",
    "\n",
    "bb_layers = [\"layer3\", \"layer4\"]\n",
    "size = 224\n",
    "batch_size = 1\n",
    "num_workers = 1\n",
    "concept_names = [\"Zero\", \"One\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\", \"Nine\"]\n",
    "# 0-Even 1-Odd\n",
    "class_list = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = get_transforms(size=size)\n",
    "test_set, test_attributes = get_dataset_with_attributes(\n",
    "    data_root=\"/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/data/MNIST_EVEN_ODD\",\n",
    "    json_root=\"/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/scripts_data\",\n",
    "    dataset_name=\"mnist\",\n",
    "    mode=\"test\",\n",
    "    attribute_file=\"attributes.npy\"\n",
    ")\n",
    "\n",
    "test_dataset = Dataset_attributes_mnist(test_set, test_attributes, transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "activations_path = os.path.join(logs, \"activations\", \"BB\", model_arch, dataset_name)\n",
    "activation_file = open(os.path.join(activations_path, \"train_cavs.pkl\"), \"rb\")\n",
    "activation = pickle.load(activation_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(model_arch, num_classes, pretrained, transfer_learning).to(device)\n",
    "checkpoint_path = os.path.join(logs, \"chk_pt\", \"BB\", model_arch, dataset_name, chk_pt_path)\n",
    "model_chk_pt = torch.load(checkpoint_path)\n",
    "model.load_state_dict(model_chk_pt)\n",
    "\n",
    "model.eval()\n",
    "model_meta = Model_Meta(model, bb_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_derivative(model_meta, cav, layer_name, class_name, outputs):\n",
    "    print(layer_name)\n",
    "    gradient = model_meta.calculate_grads(class_name, layer_name, outputs).reshape(-1)\n",
    "    return np.dot(gradient, cav) > 0\n",
    "\n",
    "\n",
    "def tcav_score(model, model_meta, data_loader, cav, layer_name, class_list, concept):\n",
    "    derivatives = {}\n",
    "    for k in class_list:\n",
    "        derivatives[k] = []\n",
    "\n",
    "    tcav_bar = tqdm(data_loader)\n",
    "    tcav_bar.set_description('Calculating tcav score for %s' % concept)\n",
    "    for x, y in tcav_bar:\n",
    "        model.eval()\n",
    "        x = x.to(device)\n",
    "        outputs = model(x)\n",
    "        k = 1 if outputs.item() >= 0.5 else 0\n",
    "        if k in class_list:\n",
    "            derivatives[k].append(directional_derivative(model_meta, cav, layer_name, k, outputs))\n",
    "\n",
    "    score = np.zeros(len(class_list))\n",
    "    for i, k in enumerate(class_list):\n",
    "        score[i] = np.count_nonzero(np.array(derivatives[k]))/len(derivatives[k])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in bb_layers:\n",
    "    print(f\"TCAV for layer: {layer}\")\n",
    "    for idx, concept in enumerate(concept_names):\n",
    "        print(f\"========>> Concept: {concept} <<=======\")\n",
    "        cavs = activation[layer][idx]\n",
    "        cav = cavs[1]\n",
    "        print(cav)\n",
    "        score = tcav_score(model, model_meta, test_dataloader, cav, layer, class_list, idx)\n",
    "        print(f\"TCAV class 0(Even): {score[0]}, class 0(Odd): {score[1]}\")\n",
    "    print(\"-------------------------------------------\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_3_7",
   "language": "python",
   "name": "python_3_7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
