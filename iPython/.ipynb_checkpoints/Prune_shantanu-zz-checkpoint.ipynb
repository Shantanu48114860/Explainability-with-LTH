{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "import torchvision.utils as vutils\n",
    "import torch.nn.init as init\n",
    "import pickle\n",
    "\n",
    "sys.path.append(os.path.abspath(\"/ocean/projects/asc170022p/shg121/PhD/Project_Pruning\"))\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import concept_activations.concept_activations_utils as ca_utils\n",
    "from model_factory.model_meta import Model_Meta\n",
    "from model_factory.models import Classifier\n",
    "from run_manager import RunManager\n",
    "from concept_activations.g import G\n",
    "\n",
    "import utils\n",
    "import yaml\n",
    "import pickle\n",
    "import torch\n",
    "from dataset.dataset_mnist import Dataset_mnist\n",
    "from dataset.dataset_utils import get_dataset, get_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "device = utils.get_device()\n",
    "print(f\"Device: {device}\")\n",
    "data_root = \"/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/data/MNIST_EVEN_ODD\"\n",
    "json_root = \"/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/scripts_data\"\n",
    "model_arch = \"Resnet_18\"\n",
    "dataset_name = \"mnist\"\n",
    "pretrained = True\n",
    "transfer_learning = False\n",
    "chk_pt_path = \"seq_epoch_20.pth.tar\"\n",
    "num_classes = 1\n",
    "logs = \"/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/output\"\n",
    "bb_layer = \"layer3\"  # layer3\n",
    "concept_names = [\"Zero\", \"One\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\", \"Nine\"]\n",
    "img_size = 224\n",
    "batch_size = 3\n",
    "epochs = 50\n",
    "num_workers = 4\n",
    "class_list = [0, 1]\n",
    "num_labels = len(class_list)\n",
    "cav_vector_file = \"max_pooled_train_cavs.pkl\"\n",
    "kernel_size={\n",
    "    \"layer3\": 14,\n",
    "    \"layer4\": 7\n",
    "}\n",
    "\n",
    "prune_type = \"lt\"\n",
    "lr = 1e-3\n",
    "prune_iterations  = 35\n",
    "prune_percent = 10\n",
    "start_iter = 0\n",
    "end_iter = 100\n",
    "resample = False\n",
    "reinit = True if prune_type==\"reinit\" else False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_dataloader(\n",
    "        dataset_name,\n",
    "        data_root,\n",
    "        json_root,\n",
    "        batch_size,\n",
    "        transform_params\n",
    "):\n",
    "    if dataset_name == \"mnist\":\n",
    "        test_set = get_dataset(\n",
    "            data_root=data_root,\n",
    "            json_root=json_root,\n",
    "            dataset_name=dataset_name,\n",
    "            mode=\"test\"\n",
    "        )\n",
    "        transform = get_transforms(size=transform_params[\"img_size\"])\n",
    "        test_dataset = Dataset_mnist(test_set, transform)\n",
    "        return DataLoader(\n",
    "            test_dataset,\n",
    "            num_workers=4,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "def load_mask(self, _ite, percent_weight_remain):\n",
    "    mask_file_name = f\"lt_mask_non_zero_params_{percent_weight_remain}_ite_{_ite}.pkl\"\n",
    "    mask_file = open(\n",
    "            os.path.join(self.mask_path, mask_file_name),\n",
    "            \"rb\"\n",
    "        )\n",
    "    mask = pickle.load(mask_file)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = num_classes\n",
    "prune_type = prune_type\n",
    "model = Classifier(model_arch, num_classes, pretrained, transfer_learning)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the [test] dataset: 60000\n",
      "prune_stat_path-for-each-prune-iteration directory is created successfully at:\n",
      "/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/output/chk_pt/Pruning/Resnet_18/mnist/prune-statistics/test\n"
     ]
    }
   ],
   "source": [
    "transform_params = {\n",
    "            \"img_size\": img_size\n",
    "}\n",
    "device = device\n",
    "checkpoint_path = os.path.join(logs, \"chk_pt\", \"Pruning\", model_arch, dataset_name)\n",
    "mask_path = os.path.join(logs, \"chk_pt\", \"Pruning\", model_arch, dataset_name, \"mask\")\n",
    "prune_stat_path = os.path.join(checkpoint_path, \"prune-statistics\", \"test\")\n",
    "\n",
    "test_loader = get_test_dataloader(\n",
    "    dataset_name,\n",
    "    data_root,\n",
    "    json_root,\n",
    "    batch_size,\n",
    "    transform_params\n",
    ")\n",
    "\n",
    "prune_statistics = []\n",
    "utils.create_dir(\n",
    "    path_dict={\n",
    "        \"path_name\": prune_stat_path,\n",
    "        \"path_type\": \"prune_stat_path-for-each-prune-iteration\"\n",
    "    })\n",
    "\n",
    "percent_weight_remaining = [\n",
    "    100.0, 90.0, 81.0, 72.9, 65.6, 59.1, 53.2, 47.8, 43.1, 38.8, 34.9, 31.4,\n",
    "    28.3, 25.4, 22.9, 20.6, 18.6, 16.7, 15.0, 13.5, 12.2, 11.0, 9.9, 8.9,\n",
    "    8.0, 7.2, 6.5, 5.9, 5.3, 4.7, 4.3, 3.9, 3.5, 3.1, 2.8,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mask(_ite, percent_weight_remain):\n",
    "    mask_file_name = f\"lt_mask_non_zero_params_{percent_weight_remain}_ite_{_ite}.pkl\"\n",
    "    mask_file = open(\n",
    "            os.path.join(mask_path, mask_file_name),\n",
    "            \"rb\"\n",
    "    )\n",
    "    mask = pickle.load(mask_file)\n",
    "    return mask\n",
    "\n",
    "def prune_by_mask(_ite, mask):\n",
    "    step = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        # Prune weights, not biases\n",
    "        if 'weight' in name:\n",
    "            tensor = param.data.cpu().numpy()\n",
    "            # Convert Tensors to numpy and calculate\n",
    "            weight_dev = param.device\n",
    "            # Apply new weight and mask\n",
    "            param.data = torch.from_numpy(tensor * mask[step]).to(weight_dev)\n",
    "            step += 1\n",
    "            \n",
    "def load_model(_ite):\n",
    "    chk_pt_file = f\"best_val_prune_iteration_{_ite}_model_lt.pth.tar\"\n",
    "    chk_pt_file_name = os.path.join(checkpoint_path, chk_pt_file)\n",
    "    model_chk_pt = torch.load(chk_pt_file_name)\n",
    "    model.load_state_dict(model_chk_pt)\n",
    "    \n",
    "def test_model(_ite):\n",
    "    model.eval()\n",
    "    g.eval()\n",
    "    bb_correct = 0\n",
    "    g_correct = 0\n",
    "    \n",
    "    out_put_GT = torch.FloatTensor().cuda()\n",
    "    out_put_predict_bb = torch.FloatTensor().cuda()\n",
    "    out_put_predict_g = torch.FloatTensor().cuda()\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(test_loader)) as t:\n",
    "            for data, target in test_loader:\n",
    "                bs = data.size(0)\n",
    "                data, target = data.to(device), target.to(torch.float32).to(device)\n",
    "                target = target.reshape((target.shape[0], 1))\n",
    "                output_bb = model(data)\n",
    "                \n",
    "                activations = bb_model_meta.model_activations_store[bb_layer]\n",
    "                norm_vc = ca_utils.get_normalized_vc(\n",
    "                    activations,\n",
    "                    torch_concept_vector,\n",
    "                    th,\n",
    "                    val_after_th,\n",
    "                    cav_flattening_type\n",
    "                )\n",
    "                concept_to_act = g(norm_vc)\n",
    "                output_g = ca_utils.get_concept_to_pred(\n",
    "                    concept_to_act,\n",
    "                    bs,\n",
    "                    activations,\n",
    "                    bb_model_mid,\n",
    "                    bb_model_tail\n",
    "                )\n",
    "                \n",
    "                bb_correct += utils.get_correct(output_bb, target)\n",
    "                g_correct += utils.get_correct(output_g, target)\n",
    "                \n",
    "                out_put_predict_bb = torch.cat((out_put_predict_bb, output_bb), dim=0)\n",
    "                out_put_predict_g = torch.cat((out_put_predict_g, output_g), dim=0)\n",
    "                out_put_GT = torch.cat((out_put_GT, target), dim=0)\n",
    "                \n",
    "                t.set_postfix(\n",
    "                    iteration=f\"{_ite}\",\n",
    "                    bb_correct=f\"{bb_correct}\",\n",
    "                    g_correct = f\"{g_correct}\"\n",
    "                )\n",
    "                t.update()\n",
    "    \n",
    "    bb_accuracy = 100. * bb_correct / len(test_loader.dataset)\n",
    "    g_accuracy = 100. * g_correct / len(test_loader.dataset)\n",
    "        \n",
    "    random_pred_acc = 1 / num_labels\n",
    "    completeness = (g_accuracy - random_pred_acc) / (bb_accuracy - random_pred_acc)\n",
    "        \n",
    "    print(f\"bb_accuracy_normal: {bb_accuracy}\")\n",
    "    print(f\"g_accuracy_normal: {g_accuracy}\")\n",
    "    print(f\"completeness_score_normal: {completeness}\")\n",
    "    \n",
    "    out_put_GT_np = out_put_GT.cpu().numpy()\n",
    "    out_put_predict_bb_np = out_put_predict_bb.cpu().numpy()\n",
    "    out_put_predict_g = out_put_predict_g.cpu().numpy()\n",
    "    y_hat_bb = np.where(out_put_predict_bb_np > 0.5, 1, 0)\n",
    "    y_hat_g = np.where(out_put_predict_g > 0.5, 1, 0)\n",
    "    \n",
    "    print(\"Accuracy using B: \")\n",
    "    acc_bb = utils.cal_accuracy(out_put_GT_np, y_hat_bb)\n",
    "    print(f\"Accuracy: {acc_bb}\")\n",
    "    print(f\"Precision: {utils.cal_precision(out_put_GT_np, y_hat_bb)}\")\n",
    "    print(f\"Recall: {utils.cal_recall(out_put_GT_np, y_hat_bb)}\")\n",
    "    print(f\"RocAUC: {utils.cal_roc_auc(out_put_GT_np, y_hat_bb)}\")\n",
    "    print(f\"F1 score: {utils.cal_f1_score(out_put_GT_np, y_hat_bb)}\")\n",
    "    \n",
    "    print(\"Accuracy using G: \")\n",
    "    acc_g = utils.cal_accuracy(out_put_GT_np, y_hat_g)\n",
    "    print(f\"Accuracy: {acc_g}\")\n",
    "    print(f\"Precision: {utils.cal_precision(out_put_GT_np, y_hat_g)}\")\n",
    "    print(f\"Recall: {utils.cal_recall(out_put_GT_np, y_hat_g)}\")\n",
    "    print(f\"RocAUC: {utils.cal_roc_auc(out_put_GT_np, y_hat_g)}\")\n",
    "    print(f\"F1 score: {utils.cal_f1_score(out_put_GT_np, y_hat_g)}\")\n",
    "    \n",
    "    random_pred_acc = 1 / num_labels\n",
    "    completeness = (acc_g - random_pred_acc) / (acc_bb - random_pred_acc)\n",
    "    print(f\"Completeness score sklearn: {completeness}\")\n",
    "\n",
    "    \n",
    "    return bb_accuracy, g_accuracy, completeness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load the concepts from disk: 0.008333206176757812 secs\n"
     ]
    }
   ],
   "source": [
    "cav_flattening_type = \"flattened\"\n",
    "g_model_chk_pt = \"best_epoch_42.pth.tar\"\n",
    "hidden_features = 500\n",
    "th = 0\n",
    "val_after_th = 0\n",
    "concept_vectors = ca_utils.get_concept_vectors(\n",
    "        logs,\n",
    "        bb_layer,\n",
    "        cav_flattening_type,\n",
    "        model_arch,\n",
    "        dataset_name\n",
    ")\n",
    "torch_concept_vector = torch.from_numpy(concept_vectors).to(device, dtype=torch.float32)\n",
    "\n",
    "g_model_checkpoint_path = os.path.join(\n",
    "        logs,\n",
    "        \"chk_pt\",\n",
    "        \"G\",\n",
    "        model_arch,\n",
    "        bb_layer,\n",
    "        dataset_name,\n",
    "        cav_flattening_type,\n",
    "        g_model_chk_pt\n",
    "    )\n",
    "\n",
    "bb_model_meta = None\n",
    "if type(bb_layer) == str:\n",
    "    bb_model_meta = Model_Meta(model, [bb_layer])\n",
    "    \n",
    "g_model_ip_size, g_model_op_size = ca_utils.get_g_model_ip_op_size(\n",
    "        test_loader,\n",
    "        device,\n",
    "        model,\n",
    "        bb_model_meta,\n",
    "        concept_vectors,\n",
    "        bb_layer,\n",
    "        cav_flattening_type\n",
    "    )\n",
    "\n",
    "g = G(g_model_ip_size, g_model_op_size, hidden_features).to(device)\n",
    "g.load_state_dict(torch.load(g_model_checkpoint_path))\n",
    "bb_model_mid, bb_model_tail = ca_utils.dissect_bb_model(model_arch, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [01:58<00:00, 168.56it/s, bb_correct=59004, g_correct=40341, iteration=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bb_accuracy_normal: 98.34\n",
      "g_accuracy_normal: 67.235\n",
      "completeness_score_normal: 0.6820829926410465\n",
      "Accuracy using B: \n",
      "Accuracy: 0.9834\n",
      "Precision: 0.9907869354087674\n",
      "Recall: 0.9764324111708405\n",
      "RocAUC: 0.9835200167884584\n",
      "F1 score: 0.983557301812659\n",
      "Accuracy using G: \n",
      "Accuracy: 0.67235\n",
      "Precision: 0.6999373410489845\n",
      "Recall: 0.6224596827061755\n",
      "RocAUC: 0.673209361222883\n",
      "F1 score: 0.658928850257638\n",
      "Completeness score sklearn: 0.35653702937525855\n",
      "Success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [01:59<00:00, 167.93it/s, bb_correct=58225, g_correct=29492, iteration=1]\n",
      "/ocean/projects/asc170022p/shg121/anaconda3/envs/python_3_7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bb_accuracy_normal: 97.04166666666667\n",
      "g_accuracy_normal: 49.153333333333336\n",
      "completeness_score_normal: 0.5039620198532585\n",
      "Accuracy using B: \n",
      "Accuracy: 0.9704166666666667\n",
      "Precision: 0.9960979315584102\n",
      "Recall: 0.9455224859053363\n",
      "RocAUC: 0.9708454691835104\n",
      "F1 score: 0.9701515126036288\n",
      "Accuracy using G: \n",
      "Accuracy: 0.4915333333333333\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "RocAUC: 0.5\n",
      "F1 score: 0.0\n",
      "Completeness score sklearn: -0.017998228520814903\n",
      "Success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [01:59<00:00, 167.20it/s, bb_correct=58262, g_correct=29493, iteration=2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bb_accuracy_normal: 97.10333333333334\n",
      "g_accuracy_normal: 49.155\n",
      "completeness_score_normal: 0.5036575687519409\n",
      "Accuracy using B: \n",
      "Accuracy: 0.9710333333333333\n",
      "Precision: 0.9499249343175279\n",
      "Recall: 0.9955093745902714\n",
      "RocAUC: 0.9706117332737062\n",
      "F1 score: 0.9721830985915494\n",
      "Accuracy using G: \n",
      "Accuracy: 0.49155\n",
      "Precision: 1.0\n",
      "Recall: 3.277828766225252e-05\n",
      "RocAUC: 0.5000163891438312\n",
      "F1 score: 6.555442656265364e-05\n",
      "Completeness score sklearn: -0.01793928242870288\n",
      "Success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [01:59<00:00, 167.75it/s, bb_correct=57358, g_correct=32090, iteration=3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bb_accuracy_normal: 95.59666666666666\n",
      "g_accuracy_normal: 53.483333333333334\n",
      "completeness_score_normal: 0.557152371271338\n",
      "Accuracy using B: \n",
      "Accuracy: 0.9559666666666666\n",
      "Precision: 0.9617857616333024\n",
      "Recall: 0.951193129670906\n",
      "RocAUC: 0.9560488908899762\n",
      "F1 score: 0.9564601186552405\n",
      "Accuracy using G: \n",
      "Accuracy: 0.5348333333333334\n",
      "Precision: 0.5223004291845493\n",
      "Recall: 0.9972466238363707\n",
      "RocAUC: 0.5268682597006349\n",
      "F1 score: 0.6855494715969264\n",
      "Completeness score sklearn: 0.07639447328021065\n",
      "Success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 17641/20000 [01:45<00:14, 167.68it/s, bb_correct=52067, g_correct=24096, iteration=4]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37118/1596624664.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprune_by_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mbb_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompleteness_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#     print(f\"bb_accuracy: {bb_accuracy}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#     print(f\"g_accuracy: {g_accuracy}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_37118/844113068.py\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(_ite)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 )\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0mbb_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_correct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_bb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                 \u001b[0mg_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_correct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/utils.py\u001b[0m in \u001b[0;36mget_correct\u001b[0;34m(y_hat, y)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/ocean/projects/asc170022p/shg121/anaconda3/envs/python_3_7/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2260\u001b[0;31m                           initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ocean/projects/asc170022p/shg121/anaconda3/envs/python_3_7/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ite in range(start_iter, prune_iterations):\n",
    "    mask = load_mask(_ite, percent_weight_remaining[_ite])\n",
    "    prune_by_mask(_ite, mask)\n",
    "    load_model(_ite)\n",
    "    bb_accuracy, g_accuracy, completeness_score = test_model(_ite)\n",
    "#     print(f\"bb_accuracy: {bb_accuracy}\")\n",
    "#     print(f\"g_accuracy: {g_accuracy}\")\n",
    "#     print(f\"completeness_score: {completeness_score}\")\n",
    "    print(\"Success\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint directory is created successfully at:\n",
      "/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/output/chk_pt/Pruning/Resnet_18/mnist\n",
      "Final list: \n",
      "name: model.conv1.weight, param_size: torch.Size([64, 3, 7, 7]), mask_size: (64, 3, 7, 7)\n",
      "name: model.bn1.weight, param_size: torch.Size([64]), mask_size: (64,)\n",
      "name: model.layer1.0.conv1.weight, param_size: torch.Size([64, 64, 3, 3]), mask_size: (64, 64, 3, 3)\n",
      "name: model.layer1.0.bn1.weight, param_size: torch.Size([64]), mask_size: (64,)\n",
      "name: model.layer1.0.conv2.weight, param_size: torch.Size([64, 64, 3, 3]), mask_size: (64, 64, 3, 3)\n",
      "name: model.layer1.0.bn2.weight, param_size: torch.Size([64]), mask_size: (64,)\n",
      "name: model.layer1.1.conv1.weight, param_size: torch.Size([64, 64, 3, 3]), mask_size: (64, 64, 3, 3)\n",
      "name: model.layer1.1.bn1.weight, param_size: torch.Size([64]), mask_size: (64,)\n",
      "name: model.layer1.1.conv2.weight, param_size: torch.Size([64, 64, 3, 3]), mask_size: (64, 64, 3, 3)\n",
      "name: model.layer1.1.bn2.weight, param_size: torch.Size([64]), mask_size: (64,)\n",
      "name: model.layer2.0.conv1.weight, param_size: torch.Size([128, 64, 3, 3]), mask_size: (128, 64, 3, 3)\n",
      "name: model.layer2.0.bn1.weight, param_size: torch.Size([128]), mask_size: (128,)\n",
      "name: model.layer2.0.conv2.weight, param_size: torch.Size([128, 128, 3, 3]), mask_size: (128, 128, 3, 3)\n",
      "name: model.layer2.0.bn2.weight, param_size: torch.Size([128]), mask_size: (128,)\n",
      "name: model.layer2.0.downsample.0.weight, param_size: torch.Size([128, 64, 1, 1]), mask_size: (128, 64, 1, 1)\n",
      "name: model.layer2.0.downsample.1.weight, param_size: torch.Size([128]), mask_size: (128,)\n",
      "name: model.layer2.1.conv1.weight, param_size: torch.Size([128, 128, 3, 3]), mask_size: (128, 128, 3, 3)\n",
      "name: model.layer2.1.bn1.weight, param_size: torch.Size([128]), mask_size: (128,)\n",
      "name: model.layer2.1.conv2.weight, param_size: torch.Size([128, 128, 3, 3]), mask_size: (128, 128, 3, 3)\n",
      "name: model.layer2.1.bn2.weight, param_size: torch.Size([128]), mask_size: (128,)\n",
      "name: model.layer3.0.conv1.weight, param_size: torch.Size([256, 128, 3, 3]), mask_size: (256, 128, 3, 3)\n",
      "name: model.layer3.0.bn1.weight, param_size: torch.Size([256]), mask_size: (256,)\n",
      "name: model.layer3.0.conv2.weight, param_size: torch.Size([256, 256, 3, 3]), mask_size: (256, 256, 3, 3)\n",
      "name: model.layer3.0.bn2.weight, param_size: torch.Size([256]), mask_size: (256,)\n",
      "name: model.layer3.0.downsample.0.weight, param_size: torch.Size([256, 128, 1, 1]), mask_size: (256, 128, 1, 1)\n",
      "name: model.layer3.0.downsample.1.weight, param_size: torch.Size([256]), mask_size: (256,)\n",
      "name: model.layer3.1.conv1.weight, param_size: torch.Size([256, 256, 3, 3]), mask_size: (256, 256, 3, 3)\n",
      "name: model.layer3.1.bn1.weight, param_size: torch.Size([256]), mask_size: (256,)\n",
      "name: model.layer3.1.conv2.weight, param_size: torch.Size([256, 256, 3, 3]), mask_size: (256, 256, 3, 3)\n",
      "name: model.layer3.1.bn2.weight, param_size: torch.Size([256]), mask_size: (256,)\n",
      "name: model.layer4.0.conv1.weight, param_size: torch.Size([512, 256, 3, 3]), mask_size: (512, 256, 3, 3)\n",
      "name: model.layer4.0.bn1.weight, param_size: torch.Size([512]), mask_size: (512,)\n",
      "name: model.layer4.0.conv2.weight, param_size: torch.Size([512, 512, 3, 3]), mask_size: (512, 512, 3, 3)\n",
      "name: model.layer4.0.bn2.weight, param_size: torch.Size([512]), mask_size: (512,)\n",
      "name: model.layer4.0.downsample.0.weight, param_size: torch.Size([512, 256, 1, 1]), mask_size: (512, 256, 1, 1)\n",
      "name: model.layer4.0.downsample.1.weight, param_size: torch.Size([512]), mask_size: (512,)\n",
      "name: model.layer4.1.conv1.weight, param_size: torch.Size([512, 512, 3, 3]), mask_size: (512, 512, 3, 3)\n",
      "name: model.layer4.1.bn1.weight, param_size: torch.Size([512]), mask_size: (512,)\n",
      "name: model.layer4.1.conv2.weight, param_size: torch.Size([512, 512, 3, 3]), mask_size: (512, 512, 3, 3)\n",
      "name: model.layer4.1.bn2.weight, param_size: torch.Size([512]), mask_size: (512,)\n",
      "name: model.fc.0.weight, param_size: torch.Size([1, 512]), mask_size: (1, 512)\n",
      "\n",
      "--- Pruning Level [1:0/35]: ---\n",
      "model.conv1.weight   | nonzeros =    9408 /    9408 (100.00%) | total_pruned =       0 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =   73728 /   73728 (100.00%) | total_pruned =       0 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =  147456 /  147456 (100.00%) | total_pruned =       0 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =    8192 /    8192 (100.00%) | total_pruned =       0 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =  147456 /  147456 (100.00%) | total_pruned =       0 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =  147456 /  147456 (100.00%) | total_pruned =       0 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =  294912 /  294912 (100.00%) | total_pruned =       0 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =  589824 /  589824 (100.00%) | total_pruned =       0 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =   32768 /   32768 (100.00%) | total_pruned =       0 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =  589824 /  589824 (100.00%) | total_pruned =       0 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =  589824 /  589824 (100.00%) | total_pruned =       0 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros = 1179648 / 1179648 (100.00%) | total_pruned =       0 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros = 2359296 / 2359296 (100.00%) | total_pruned =       0 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =  131072 /  131072 (100.00%) | total_pruned =       0 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros = 2359296 / 2359296 (100.00%) | total_pruned =       0 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros = 2359296 / 2359296 (100.00%) | total_pruned =       0 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 11172225, pruned : 4800, total: 11177025, Compression rate :       1.00x  (  0.04% pruned)\n",
      "[100.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0222, -0.0229, -0.0192, -0.0139, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0205, -0.0163,  0.0025, -0.0041,  0.0255,  0.0099, -0.0151],\n",
      "        [-0.0128, -0.0167,  0.0057,  0.0339, -0.0145, -0.0111,  0.0226],\n",
      "        [-0.0302, -0.0333, -0.0135,  0.0088, -0.0024, -0.0007, -0.0027],\n",
      "        [-0.0052,  0.0160,  0.0232,  0.0178,  0.0081, -0.0048,  0.0227],\n",
      "        [-0.0412,  0.0238, -0.0183,  0.0189, -0.0133, -0.0059,  0.0416],\n",
      "        [-0.0238, -0.0099,  0.0234, -0.0054,  0.0111,  0.0097, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "1\n",
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0222, -0.0229, -0.0192, -0.0139, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0205, -0.0163,  0.0000, -0.0041,  0.0255,  0.0099, -0.0151],\n",
      "        [-0.0128, -0.0167,  0.0057,  0.0339, -0.0145, -0.0111,  0.0226],\n",
      "        [-0.0302, -0.0333, -0.0135,  0.0088, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0052,  0.0160,  0.0232,  0.0178,  0.0081, -0.0048,  0.0227],\n",
      "        [-0.0412,  0.0238, -0.0183,  0.0189, -0.0133, -0.0059,  0.0416],\n",
      "        [-0.0238, -0.0099,  0.0234, -0.0054,  0.0111,  0.0097, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:1/35]: ---\n",
      "model.conv1.weight   | nonzeros =    8467 /    9408 ( 90.00%) | total_pruned =     941 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =   33177 /   36864 ( 90.00%) | total_pruned =    3687 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =   33177 /   36864 ( 90.00%) | total_pruned =    3687 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =   33177 /   36864 ( 90.00%) | total_pruned =    3687 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =   33177 /   36864 ( 90.00%) | total_pruned =    3687 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =   66355 /   73728 ( 90.00%) | total_pruned =    7373 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =  132710 /  147456 ( 90.00%) | total_pruned =   14746 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =    7372 /    8192 ( 89.99%) | total_pruned =     820 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =  132710 /  147456 ( 90.00%) | total_pruned =   14746 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =  132710 /  147456 ( 90.00%) | total_pruned =   14746 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =  265420 /  294912 ( 90.00%) | total_pruned =   29492 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =  530841 /  589824 ( 90.00%) | total_pruned =   58983 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =   29491 /   32768 ( 90.00%) | total_pruned =    3277 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =  530841 /  589824 ( 90.00%) | total_pruned =   58983 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =  530841 /  589824 ( 90.00%) | total_pruned =   58983 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros = 1061683 / 1179648 ( 90.00%) | total_pruned =  117965 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros = 2123366 / 2359296 ( 90.00%) | total_pruned =  235930 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =  117964 /  131072 ( 90.00%) | total_pruned =   13108 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros = 2123366 / 2359296 ( 90.00%) | total_pruned =  235930 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros = 2123366 / 2359296 ( 90.00%) | total_pruned =  235930 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 10054982, pruned : 1122043, total: 11177025, Compression rate :       1.11x  ( 10.04% pruned)\n",
      "[100.  90.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0222, -0.0229, -0.0192, -0.0139, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0205, -0.0163,  0.0000, -0.0041,  0.0255,  0.0099, -0.0151],\n",
      "        [-0.0128, -0.0167,  0.0057,  0.0339, -0.0145, -0.0111,  0.0226],\n",
      "        [-0.0302, -0.0333, -0.0135,  0.0088, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0052,  0.0160,  0.0232,  0.0178,  0.0081, -0.0048,  0.0227],\n",
      "        [-0.0412,  0.0238, -0.0183,  0.0189, -0.0133, -0.0059,  0.0416],\n",
      "        [-0.0238, -0.0099,  0.0234, -0.0054,  0.0111,  0.0097, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0222, -0.0229, -0.0192, -0.0139, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0205, -0.0163,  0.0000, -0.0000,  0.0255,  0.0099, -0.0151],\n",
      "        [-0.0128, -0.0167,  0.0000,  0.0339, -0.0145, -0.0111,  0.0226],\n",
      "        [-0.0302, -0.0333, -0.0135,  0.0088, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0160,  0.0232,  0.0178,  0.0081, -0.0000,  0.0227],\n",
      "        [-0.0412,  0.0238, -0.0183,  0.0189, -0.0133, -0.0000,  0.0416],\n",
      "        [-0.0238, -0.0099,  0.0234, -0.0000,  0.0111,  0.0097, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:2/35]: ---\n",
      "model.conv1.weight   | nonzeros =    7620 /    9408 ( 80.99%) | total_pruned =    1788 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =      51 /      64 ( 79.69%) | total_pruned =      13 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =   29859 /   36864 ( 81.00%) | total_pruned =    7005 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =      51 /      64 ( 79.69%) | total_pruned =      13 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =   29859 /   36864 ( 81.00%) | total_pruned =    7005 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =      51 /      64 ( 79.69%) | total_pruned =      13 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =   29859 /   36864 ( 81.00%) | total_pruned =    7005 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =      51 /      64 ( 79.69%) | total_pruned =      13 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =   29859 /   36864 ( 81.00%) | total_pruned =    7005 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =      51 /      64 ( 79.69%) | total_pruned =      13 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =   59719 /   73728 ( 81.00%) | total_pruned =   14009 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =     103 /     128 ( 80.47%) | total_pruned =      25 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =  119439 /  147456 ( 81.00%) | total_pruned =   28017 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =     103 /     128 ( 80.47%) | total_pruned =      25 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =    6634 /    8192 ( 80.98%) | total_pruned =    1558 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =     103 /     128 ( 80.47%) | total_pruned =      25 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =  119439 /  147456 ( 81.00%) | total_pruned =   28017 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =     103 /     128 ( 80.47%) | total_pruned =      25 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =  119439 /  147456 ( 81.00%) | total_pruned =   28017 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =     103 /     128 ( 80.47%) | total_pruned =      25 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =  238878 /  294912 ( 81.00%) | total_pruned =   56034 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =     207 /     256 ( 80.86%) | total_pruned =      49 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =  477757 /  589824 ( 81.00%) | total_pruned =  112067 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =     207 /     256 ( 80.86%) | total_pruned =      49 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =   26542 /   32768 ( 81.00%) | total_pruned =    6226 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =     207 /     256 ( 80.86%) | total_pruned =      49 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =  477757 /  589824 ( 81.00%) | total_pruned =  112067 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =     207 /     256 ( 80.86%) | total_pruned =      49 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =  477757 /  589824 ( 81.00%) | total_pruned =  112067 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =     207 /     256 ( 80.86%) | total_pruned =      49 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =  955514 / 1179648 ( 81.00%) | total_pruned =  224134 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =     414 /     512 ( 80.86%) | total_pruned =      98 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros = 1911029 / 2359296 ( 81.00%) | total_pruned =  448267 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =     414 /     512 ( 80.86%) | total_pruned =      98 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =  106167 /  131072 ( 81.00%) | total_pruned =   24905 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =     414 /     512 ( 80.86%) | total_pruned =      98 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros = 1911029 / 2359296 ( 81.00%) | total_pruned =  448267 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =     414 /     512 ( 80.86%) | total_pruned =      98 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros = 1911029 / 2359296 ( 81.00%) | total_pruned =  448267 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =     414 /     512 ( 80.86%) | total_pruned =      98 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =     414 /     512 ( 80.86%) | total_pruned =      98 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 9049475, pruned : 2127550, total: 11177025, Compression rate :       1.24x  ( 19.04% pruned)\n",
      "[100.  90.  81.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0222, -0.0229, -0.0192, -0.0139, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0205, -0.0163,  0.0000, -0.0000,  0.0255,  0.0099, -0.0151],\n",
      "        [-0.0128, -0.0167,  0.0000,  0.0339, -0.0145, -0.0111,  0.0226],\n",
      "        [-0.0302, -0.0333, -0.0135,  0.0088, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0160,  0.0232,  0.0178,  0.0081, -0.0000,  0.0227],\n",
      "        [-0.0412,  0.0238, -0.0183,  0.0189, -0.0133, -0.0000,  0.0416],\n",
      "        [-0.0238, -0.0099,  0.0234, -0.0000,  0.0111,  0.0097, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0222, -0.0229, -0.0192, -0.0139, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0205, -0.0163,  0.0000, -0.0000,  0.0255,  0.0099, -0.0151],\n",
      "        [-0.0128, -0.0167,  0.0000,  0.0339, -0.0145, -0.0111,  0.0226],\n",
      "        [-0.0302, -0.0333, -0.0135,  0.0088, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0160,  0.0232,  0.0178,  0.0000, -0.0000,  0.0227],\n",
      "        [-0.0412,  0.0238, -0.0183,  0.0189, -0.0133, -0.0000,  0.0416],\n",
      "        [-0.0238, -0.0099,  0.0234, -0.0000,  0.0111,  0.0097, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:3/35]: ---\n",
      "model.conv1.weight   | nonzeros =    6858 /    9408 ( 72.90%) | total_pruned =    2550 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =      46 /      64 ( 71.88%) | total_pruned =      18 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =   26873 /   36864 ( 72.90%) | total_pruned =    9991 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =      46 /      64 ( 71.88%) | total_pruned =      18 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =   26873 /   36864 ( 72.90%) | total_pruned =    9991 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =      46 /      64 ( 71.88%) | total_pruned =      18 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =   26873 /   36864 ( 72.90%) | total_pruned =    9991 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =      46 /      64 ( 71.88%) | total_pruned =      18 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =   26873 /   36864 ( 72.90%) | total_pruned =    9991 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =      46 /      64 ( 71.88%) | total_pruned =      18 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =   53747 /   73728 ( 72.90%) | total_pruned =   19981 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =      92 /     128 ( 71.88%) | total_pruned =      36 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =  107495 /  147456 ( 72.90%) | total_pruned =   39961 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =      92 /     128 ( 71.88%) | total_pruned =      36 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =    5970 /    8192 ( 72.88%) | total_pruned =    2222 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =      92 /     128 ( 71.88%) | total_pruned =      36 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =  107495 /  147456 ( 72.90%) | total_pruned =   39961 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =      92 /     128 ( 71.88%) | total_pruned =      36 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =  107495 /  147456 ( 72.90%) | total_pruned =   39961 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =      92 /     128 ( 71.88%) | total_pruned =      36 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =  214990 /  294912 ( 72.90%) | total_pruned =   79922 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =     186 /     256 ( 72.66%) | total_pruned =      70 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =  429981 /  589824 ( 72.90%) | total_pruned =  159843 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =     186 /     256 ( 72.66%) | total_pruned =      70 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =   23887 /   32768 ( 72.90%) | total_pruned =    8881 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =     186 /     256 ( 72.66%) | total_pruned =      70 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =  429981 /  589824 ( 72.90%) | total_pruned =  159843 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =     186 /     256 ( 72.66%) | total_pruned =      70 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =  429981 /  589824 ( 72.90%) | total_pruned =  159843 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =     186 /     256 ( 72.66%) | total_pruned =      70 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =  859962 / 1179648 ( 72.90%) | total_pruned =  319686 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =     372 /     512 ( 72.66%) | total_pruned =     140 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros = 1719926 / 2359296 ( 72.90%) | total_pruned =  639370 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =     372 /     512 ( 72.66%) | total_pruned =     140 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =   95550 /  131072 ( 72.90%) | total_pruned =   35522 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =     372 /     512 ( 72.66%) | total_pruned =     140 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros = 1719926 / 2359296 ( 72.90%) | total_pruned =  639370 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =     372 /     512 ( 72.66%) | total_pruned =     140 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros = 1719926 / 2359296 ( 72.90%) | total_pruned =  639370 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =     372 /     512 ( 72.66%) | total_pruned =     140 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =     372 /     512 ( 72.66%) | total_pruned =     140 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 8144515, pruned : 3032510, total: 11177025, Compression rate :       1.37x  ( 27.13% pruned)\n",
      "[100.   90.   81.   72.9   0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0222, -0.0229, -0.0192, -0.0139, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0205, -0.0163,  0.0000, -0.0000,  0.0255,  0.0099, -0.0151],\n",
      "        [-0.0128, -0.0167,  0.0000,  0.0339, -0.0145, -0.0111,  0.0226],\n",
      "        [-0.0302, -0.0333, -0.0135,  0.0088, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0160,  0.0232,  0.0178,  0.0000, -0.0000,  0.0227],\n",
      "        [-0.0412,  0.0238, -0.0183,  0.0189, -0.0133, -0.0000,  0.0416],\n",
      "        [-0.0238, -0.0099,  0.0234, -0.0000,  0.0111,  0.0097, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0222, -0.0229, -0.0192, -0.0139, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0205, -0.0163,  0.0000, -0.0000,  0.0255,  0.0000, -0.0151],\n",
      "        [-0.0128, -0.0167,  0.0000,  0.0339, -0.0145, -0.0000,  0.0226],\n",
      "        [-0.0302, -0.0333, -0.0135,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0160,  0.0232,  0.0178,  0.0000, -0.0000,  0.0227],\n",
      "        [-0.0412,  0.0238, -0.0183,  0.0189, -0.0133, -0.0000,  0.0416],\n",
      "        [-0.0238, -0.0000,  0.0234, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:4/35]: ---\n",
      "model.conv1.weight   | nonzeros =    6172 /    9408 ( 65.60%) | total_pruned =    3236 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =      41 /      64 ( 64.06%) | total_pruned =      23 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =   24185 /   36864 ( 65.61%) | total_pruned =   12679 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =      41 /      64 ( 64.06%) | total_pruned =      23 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =   24185 /   36864 ( 65.61%) | total_pruned =   12679 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =      41 /      64 ( 64.06%) | total_pruned =      23 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =   24185 /   36864 ( 65.61%) | total_pruned =   12679 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =      41 /      64 ( 64.06%) | total_pruned =      23 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =   24185 /   36864 ( 65.61%) | total_pruned =   12679 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =      41 /      64 ( 64.06%) | total_pruned =      23 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =   48372 /   73728 ( 65.61%) | total_pruned =   25356 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =      82 /     128 ( 64.06%) | total_pruned =      46 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =   96745 /  147456 ( 65.61%) | total_pruned =   50711 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =      82 /     128 ( 64.06%) | total_pruned =      46 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =    5373 /    8192 ( 65.59%) | total_pruned =    2819 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =      82 /     128 ( 64.06%) | total_pruned =      46 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =   96745 /  147456 ( 65.61%) | total_pruned =   50711 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =      82 /     128 ( 64.06%) | total_pruned =      46 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =   96745 /  147456 ( 65.61%) | total_pruned =   50711 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =      82 /     128 ( 64.06%) | total_pruned =      46 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =  193491 /  294912 ( 65.61%) | total_pruned =  101421 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =     167 /     256 ( 65.23%) | total_pruned =      89 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =  386983 /  589824 ( 65.61%) | total_pruned =  202841 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =     167 /     256 ( 65.23%) | total_pruned =      89 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =   21498 /   32768 ( 65.61%) | total_pruned =   11270 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =     167 /     256 ( 65.23%) | total_pruned =      89 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =  386983 /  589824 ( 65.61%) | total_pruned =  202841 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =     167 /     256 ( 65.23%) | total_pruned =      89 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =  386983 /  589824 ( 65.61%) | total_pruned =  202841 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =     167 /     256 ( 65.23%) | total_pruned =      89 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =  773965 / 1179648 ( 65.61%) | total_pruned =  405683 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =     334 /     512 ( 65.23%) | total_pruned =     178 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros = 1547933 / 2359296 ( 65.61%) | total_pruned =  811363 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =     334 /     512 ( 65.23%) | total_pruned =     178 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =   85995 /  131072 ( 65.61%) | total_pruned =   45077 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =     334 /     512 ( 65.23%) | total_pruned =     178 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros = 1547934 / 2359296 ( 65.61%) | total_pruned =  811362 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =     334 /     512 ( 65.23%) | total_pruned =     178 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros = 1547933 / 2359296 ( 65.61%) | total_pruned =  811363 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =     334 /     512 ( 65.23%) | total_pruned =     178 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =     334 /     512 ( 65.23%) | total_pruned =     178 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 7330045, pruned : 3846980, total: 11177025, Compression rate :       1.52x  ( 34.42% pruned)\n",
      "[100.   90.   81.   72.9  65.6   0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0222, -0.0229, -0.0192, -0.0139, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0205, -0.0163,  0.0000, -0.0000,  0.0255,  0.0000, -0.0151],\n",
      "        [-0.0128, -0.0167,  0.0000,  0.0339, -0.0145, -0.0000,  0.0226],\n",
      "        [-0.0302, -0.0333, -0.0135,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0160,  0.0232,  0.0178,  0.0000, -0.0000,  0.0227],\n",
      "        [-0.0412,  0.0238, -0.0183,  0.0189, -0.0133, -0.0000,  0.0416],\n",
      "        [-0.0238, -0.0000,  0.0234, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0222, -0.0229, -0.0192, -0.0139, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0205, -0.0163,  0.0000, -0.0000,  0.0255,  0.0000, -0.0151],\n",
      "        [-0.0000, -0.0167,  0.0000,  0.0339, -0.0145, -0.0000,  0.0226],\n",
      "        [-0.0302, -0.0333, -0.0135,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0160,  0.0232,  0.0178,  0.0000, -0.0000,  0.0227],\n",
      "        [-0.0412,  0.0238, -0.0183,  0.0189, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0238, -0.0000,  0.0234, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:5/35]: ---\n",
      "model.conv1.weight   | nonzeros =    5554 /    9408 ( 59.03%) | total_pruned =    3854 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =      37 /      64 ( 57.81%) | total_pruned =      27 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =   21766 /   36864 ( 59.04%) | total_pruned =   15098 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =      37 /      64 ( 57.81%) | total_pruned =      27 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =   21766 /   36864 ( 59.04%) | total_pruned =   15098 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =      37 /      64 ( 57.81%) | total_pruned =      27 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =   21766 /   36864 ( 59.04%) | total_pruned =   15098 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =      37 /      64 ( 57.81%) | total_pruned =      27 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =   21766 /   36864 ( 59.04%) | total_pruned =   15098 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =      37 /      64 ( 57.81%) | total_pruned =      27 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =   43534 /   73728 ( 59.05%) | total_pruned =   30194 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =      73 /     128 ( 57.03%) | total_pruned =      55 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =   87070 /  147456 ( 59.05%) | total_pruned =   60386 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =      73 /     128 ( 57.03%) | total_pruned =      55 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =    4835 /    8192 ( 59.02%) | total_pruned =    3357 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =      73 /     128 ( 57.03%) | total_pruned =      55 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =   87070 /  147456 ( 59.05%) | total_pruned =   60386 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =      73 /     128 ( 57.03%) | total_pruned =      55 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =   87070 /  147456 ( 59.05%) | total_pruned =   60386 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =      73 /     128 ( 57.03%) | total_pruned =      55 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =  174142 /  294912 ( 59.05%) | total_pruned =  120770 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =     150 /     256 ( 58.59%) | total_pruned =     106 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =  348284 /  589824 ( 59.05%) | total_pruned =  241540 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =     150 /     256 ( 58.59%) | total_pruned =     106 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =   19348 /   32768 ( 59.05%) | total_pruned =   13420 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =     150 /     256 ( 58.59%) | total_pruned =     106 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =  348284 /  589824 ( 59.05%) | total_pruned =  241540 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =     150 /     256 ( 58.59%) | total_pruned =     106 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =  348284 /  589824 ( 59.05%) | total_pruned =  241540 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =     150 /     256 ( 58.59%) | total_pruned =     106 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =  696568 / 1179648 ( 59.05%) | total_pruned =  483080 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =     300 /     512 ( 58.59%) | total_pruned =     212 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros = 1393139 / 2359296 ( 59.05%) | total_pruned =  966157 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =     300 /     512 ( 58.59%) | total_pruned =     212 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =   77395 /  131072 ( 59.05%) | total_pruned =   53677 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =     300 /     512 ( 58.59%) | total_pruned =     212 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros = 1393140 / 2359296 ( 59.05%) | total_pruned =  966156 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =     300 /     512 ( 58.59%) | total_pruned =     212 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros = 1393139 / 2359296 ( 59.05%) | total_pruned =  966157 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =     300 /     512 ( 58.59%) | total_pruned =     212 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =     300 /     512 ( 58.59%) | total_pruned =     212 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 6597021, pruned : 4580004, total: 11177025, Compression rate :       1.69x  ( 40.98% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0222, -0.0229, -0.0192, -0.0139, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0205, -0.0163,  0.0000, -0.0000,  0.0255,  0.0000, -0.0151],\n",
      "        [-0.0000, -0.0167,  0.0000,  0.0339, -0.0145, -0.0000,  0.0226],\n",
      "        [-0.0302, -0.0333, -0.0135,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0160,  0.0232,  0.0178,  0.0000, -0.0000,  0.0227],\n",
      "        [-0.0412,  0.0238, -0.0183,  0.0189, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0238, -0.0000,  0.0234, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0222, -0.0229, -0.0192, -0.0000, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0205, -0.0163,  0.0000, -0.0000,  0.0255,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0167,  0.0000,  0.0339, -0.0000, -0.0000,  0.0226],\n",
      "        [-0.0302, -0.0333, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0160,  0.0232,  0.0178,  0.0000, -0.0000,  0.0227],\n",
      "        [-0.0412,  0.0238, -0.0183,  0.0189, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0238, -0.0000,  0.0234, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:6/35]: ---\n",
      "model.conv1.weight   | nonzeros =    4998 /    9408 ( 53.12%) | total_pruned =    4410 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =      33 /      64 ( 51.56%) | total_pruned =      31 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =   19589 /   36864 ( 53.14%) | total_pruned =   17275 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =      33 /      64 ( 51.56%) | total_pruned =      31 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =   19589 /   36864 ( 53.14%) | total_pruned =   17275 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =      33 /      64 ( 51.56%) | total_pruned =      31 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =   19589 /   36864 ( 53.14%) | total_pruned =   17275 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =      33 /      64 ( 51.56%) | total_pruned =      31 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =   19589 /   36864 ( 53.14%) | total_pruned =   17275 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =      33 /      64 ( 51.56%) | total_pruned =      31 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =   39180 /   73728 ( 53.14%) | total_pruned =   34548 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =      65 /     128 ( 50.78%) | total_pruned =      63 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =   78363 /  147456 ( 53.14%) | total_pruned =   69093 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =      65 /     128 ( 50.78%) | total_pruned =      63 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =    4351 /    8192 ( 53.11%) | total_pruned =    3841 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =      65 /     128 ( 50.78%) | total_pruned =      63 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =   78363 /  147456 ( 53.14%) | total_pruned =   69093 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =      65 /     128 ( 50.78%) | total_pruned =      63 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =   78363 /  147456 ( 53.14%) | total_pruned =   69093 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =      65 /     128 ( 50.78%) | total_pruned =      63 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =  156727 /  294912 ( 53.14%) | total_pruned =  138185 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =     135 /     256 ( 52.73%) | total_pruned =     121 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =  313455 /  589824 ( 53.14%) | total_pruned =  276369 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =     135 /     256 ( 52.73%) | total_pruned =     121 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =   17413 /   32768 ( 53.14%) | total_pruned =   15355 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =     135 /     256 ( 52.73%) | total_pruned =     121 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =  313455 /  589824 ( 53.14%) | total_pruned =  276369 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =     135 /     256 ( 52.73%) | total_pruned =     121 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =  313455 /  589824 ( 53.14%) | total_pruned =  276369 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =     135 /     256 ( 52.73%) | total_pruned =     121 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =  626911 / 1179648 ( 53.14%) | total_pruned =  552737 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =     270 /     512 ( 52.73%) | total_pruned =     242 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros = 1253825 / 2359296 ( 53.14%) | total_pruned = 1105471 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =     270 /     512 ( 52.73%) | total_pruned =     242 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =   69655 /  131072 ( 53.14%) | total_pruned =   61417 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =     270 /     512 ( 52.73%) | total_pruned =     242 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros = 1253826 / 2359296 ( 53.14%) | total_pruned = 1105470 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =     270 /     512 ( 52.73%) | total_pruned =     242 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros = 1253825 / 2359296 ( 53.14%) | total_pruned = 1105471 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =     270 /     512 ( 52.73%) | total_pruned =     242 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =     270 /     512 ( 52.73%) | total_pruned =     242 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 5937307, pruned : 5239718, total: 11177025, Compression rate :       1.88x  ( 46.88% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1   0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0222, -0.0229, -0.0192, -0.0000, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0205, -0.0163,  0.0000, -0.0000,  0.0255,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0167,  0.0000,  0.0339, -0.0000, -0.0000,  0.0226],\n",
      "        [-0.0302, -0.0333, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0160,  0.0232,  0.0178,  0.0000, -0.0000,  0.0227],\n",
      "        [-0.0412,  0.0238, -0.0183,  0.0189, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0238, -0.0000,  0.0234, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0222, -0.0229, -0.0192, -0.0000, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0205, -0.0000,  0.0000, -0.0000,  0.0255,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0339, -0.0000, -0.0000,  0.0226],\n",
      "        [-0.0302, -0.0333, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0232,  0.0000,  0.0000, -0.0000,  0.0227],\n",
      "        [-0.0412,  0.0238, -0.0183,  0.0189, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0238, -0.0000,  0.0234, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:7/35]: ---\n",
      "model.conv1.weight   | nonzeros =    4498 /    9408 ( 47.81%) | total_pruned =    4910 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =      29 /      64 ( 45.31%) | total_pruned =      35 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =   17630 /   36864 ( 47.82%) | total_pruned =   19234 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =      29 /      64 ( 45.31%) | total_pruned =      35 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =   17630 /   36864 ( 47.82%) | total_pruned =   19234 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =      29 /      64 ( 45.31%) | total_pruned =      35 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =   17630 /   36864 ( 47.82%) | total_pruned =   19234 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =      29 /      64 ( 45.31%) | total_pruned =      35 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =   17630 /   36864 ( 47.82%) | total_pruned =   19234 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =      29 /      64 ( 45.31%) | total_pruned =      35 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =   35262 /   73728 ( 47.83%) | total_pruned =   38466 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =      58 /     128 ( 45.31%) | total_pruned =      70 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =   70526 /  147456 ( 47.83%) | total_pruned =   76930 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =      58 /     128 ( 45.31%) | total_pruned =      70 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =    3916 /    8192 ( 47.80%) | total_pruned =    4276 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =      58 /     128 ( 45.31%) | total_pruned =      70 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =   70526 /  147456 ( 47.83%) | total_pruned =   76930 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =      58 /     128 ( 45.31%) | total_pruned =      70 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =   70526 /  147456 ( 47.83%) | total_pruned =   76930 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =      58 /     128 ( 45.31%) | total_pruned =      70 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =  141054 /  294912 ( 47.83%) | total_pruned =  153858 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =     121 /     256 ( 47.27%) | total_pruned =     135 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =  282109 /  589824 ( 47.83%) | total_pruned =  307715 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =     121 /     256 ( 47.27%) | total_pruned =     135 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =   15671 /   32768 ( 47.82%) | total_pruned =   17097 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =     121 /     256 ( 47.27%) | total_pruned =     135 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =  282109 /  589824 ( 47.83%) | total_pruned =  307715 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =     121 /     256 ( 47.27%) | total_pruned =     135 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =  282109 /  589824 ( 47.83%) | total_pruned =  307715 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =     121 /     256 ( 47.27%) | total_pruned =     135 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =  564220 / 1179648 ( 47.83%) | total_pruned =  615428 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =     243 /     512 ( 47.46%) | total_pruned =     269 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros = 1128442 / 2359296 ( 47.83%) | total_pruned = 1230854 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =     243 /     512 ( 47.46%) | total_pruned =     269 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =   62689 /  131072 ( 47.83%) | total_pruned =   68383 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =     243 /     512 ( 47.46%) | total_pruned =     269 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros = 1128443 / 2359296 ( 47.83%) | total_pruned = 1230853 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =     243 /     512 ( 47.46%) | total_pruned =     269 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros = 1128442 / 2359296 ( 47.83%) | total_pruned = 1230854 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =     243 /     512 ( 47.46%) | total_pruned =     269 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =     243 /     512 ( 47.46%) | total_pruned =     269 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 5343561, pruned : 5833464, total: 11177025, Compression rate :       2.09x  ( 52.19% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8   0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0222, -0.0229, -0.0192, -0.0000, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0205, -0.0000,  0.0000, -0.0000,  0.0255,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0339, -0.0000, -0.0000,  0.0226],\n",
      "        [-0.0302, -0.0333, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0232,  0.0000,  0.0000, -0.0000,  0.0227],\n",
      "        [-0.0412,  0.0238, -0.0183,  0.0189, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0238, -0.0000,  0.0234, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0222, -0.0229, -0.0000, -0.0000, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0205, -0.0000,  0.0000, -0.0000,  0.0255,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0339, -0.0000, -0.0000,  0.0226],\n",
      "        [-0.0302, -0.0333, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0232,  0.0000,  0.0000, -0.0000,  0.0227],\n",
      "        [-0.0412,  0.0238, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0238, -0.0000,  0.0234, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:8/35]: ---\n",
      "model.conv1.weight   | nonzeros =    4048 /    9408 ( 43.03%) | total_pruned =    5360 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =      26 /      64 ( 40.62%) | total_pruned =      38 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =   15867 /   36864 ( 43.04%) | total_pruned =   20997 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =      26 /      64 ( 40.62%) | total_pruned =      38 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =   15867 /   36864 ( 43.04%) | total_pruned =   20997 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =      26 /      64 ( 40.62%) | total_pruned =      38 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =   15867 /   36864 ( 43.04%) | total_pruned =   20997 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =      26 /      64 ( 40.62%) | total_pruned =      38 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =   15867 /   36864 ( 43.04%) | total_pruned =   20997 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =      26 /      64 ( 40.62%) | total_pruned =      38 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =   31735 /   73728 ( 43.04%) | total_pruned =   41993 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =      52 /     128 ( 40.62%) | total_pruned =      76 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =   63473 /  147456 ( 43.05%) | total_pruned =   83983 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =      52 /     128 ( 40.62%) | total_pruned =      76 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =    3524 /    8192 ( 43.02%) | total_pruned =    4668 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =      52 /     128 ( 40.62%) | total_pruned =      76 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =   63473 /  147456 ( 43.05%) | total_pruned =   83983 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =      52 /     128 ( 40.62%) | total_pruned =      76 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =   63473 /  147456 ( 43.05%) | total_pruned =   83983 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =      52 /     128 ( 40.62%) | total_pruned =      76 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =  126948 /  294912 ( 43.05%) | total_pruned =  167964 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =     109 /     256 ( 42.58%) | total_pruned =     147 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =  253898 /  589824 ( 43.05%) | total_pruned =  335926 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =     109 /     256 ( 42.58%) | total_pruned =     147 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =   14104 /   32768 ( 43.04%) | total_pruned =   18664 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =     109 /     256 ( 42.58%) | total_pruned =     147 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =  253898 /  589824 ( 43.05%) | total_pruned =  335926 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =     109 /     256 ( 42.58%) | total_pruned =     147 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =  253898 /  589824 ( 43.05%) | total_pruned =  335926 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =     109 /     256 ( 42.58%) | total_pruned =     147 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =  507798 / 1179648 ( 43.05%) | total_pruned =  671850 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =     218 /     512 ( 42.58%) | total_pruned =     294 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros = 1015597 / 2359296 ( 43.05%) | total_pruned = 1343699 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =     218 /     512 ( 42.58%) | total_pruned =     294 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =   56420 /  131072 ( 43.05%) | total_pruned =   74652 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =     218 /     512 ( 42.58%) | total_pruned =     294 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros = 1015598 / 2359296 ( 43.05%) | total_pruned = 1343698 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =     218 /     512 ( 42.58%) | total_pruned =     294 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros = 1015597 / 2359296 ( 43.05%) | total_pruned = 1343699 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =     218 /     512 ( 42.58%) | total_pruned =     294 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =     218 /     512 ( 42.58%) | total_pruned =     294 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 4809194, pruned : 6367831, total: 11177025, Compression rate :       2.32x  ( 56.97% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0222, -0.0229, -0.0000, -0.0000, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0205, -0.0000,  0.0000, -0.0000,  0.0255,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0339, -0.0000, -0.0000,  0.0226],\n",
      "        [-0.0302, -0.0333, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0232,  0.0000,  0.0000, -0.0000,  0.0227],\n",
      "        [-0.0412,  0.0238, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0238, -0.0000,  0.0234, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0222, -0.0229, -0.0000, -0.0000, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0255,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0339, -0.0000, -0.0000,  0.0226],\n",
      "        [-0.0302, -0.0333, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0232,  0.0000,  0.0000, -0.0000,  0.0227],\n",
      "        [-0.0412,  0.0238, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0238, -0.0000,  0.0234, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:9/35]: ---\n",
      "model.conv1.weight   | nonzeros =    3643 /    9408 ( 38.72%) | total_pruned =    5765 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =      23 /      64 ( 35.94%) | total_pruned =      41 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =   14280 /   36864 ( 38.74%) | total_pruned =   22584 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =      23 /      64 ( 35.94%) | total_pruned =      41 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =   14280 /   36864 ( 38.74%) | total_pruned =   22584 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =      23 /      64 ( 35.94%) | total_pruned =      41 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =   14280 /   36864 ( 38.74%) | total_pruned =   22584 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =      23 /      64 ( 35.94%) | total_pruned =      41 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =   14280 /   36864 ( 38.74%) | total_pruned =   22584 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =      23 /      64 ( 35.94%) | total_pruned =      41 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =   28561 /   73728 ( 38.74%) | total_pruned =   45167 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =      46 /     128 ( 35.94%) | total_pruned =      82 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =   57125 /  147456 ( 38.74%) | total_pruned =   90331 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =      46 /     128 ( 35.94%) | total_pruned =      82 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =    3171 /    8192 ( 38.71%) | total_pruned =    5021 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =      46 /     128 ( 35.94%) | total_pruned =      82 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =   57125 /  147456 ( 38.74%) | total_pruned =   90331 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =      46 /     128 ( 35.94%) | total_pruned =      82 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =   57125 /  147456 ( 38.74%) | total_pruned =   90331 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =      46 /     128 ( 35.94%) | total_pruned =      82 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =  114253 /  294912 ( 38.74%) | total_pruned =  180659 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =      98 /     256 ( 38.28%) | total_pruned =     158 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =  228508 /  589824 ( 38.74%) | total_pruned =  361316 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =      98 /     256 ( 38.28%) | total_pruned =     158 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =   12693 /   32768 ( 38.74%) | total_pruned =   20075 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =      98 /     256 ( 38.28%) | total_pruned =     158 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =  228508 /  589824 ( 38.74%) | total_pruned =  361316 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =      98 /     256 ( 38.28%) | total_pruned =     158 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =  228508 /  589824 ( 38.74%) | total_pruned =  361316 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =      98 /     256 ( 38.28%) | total_pruned =     158 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =  457018 / 1179648 ( 38.74%) | total_pruned =  722630 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =     196 /     512 ( 38.28%) | total_pruned =     316 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros =  914037 / 2359296 ( 38.74%) | total_pruned = 1445259 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =     196 /     512 ( 38.28%) | total_pruned =     316 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =   50778 /  131072 ( 38.74%) | total_pruned =   80294 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =     196 /     512 ( 38.28%) | total_pruned =     316 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =  914038 / 2359296 ( 38.74%) | total_pruned = 1445258 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =     196 /     512 ( 38.28%) | total_pruned =     316 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =  914037 / 2359296 ( 38.74%) | total_pruned = 1445259 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =     196 /     512 ( 38.28%) | total_pruned =     316 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =     196 /     512 ( 38.28%) | total_pruned =     316 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 4328260, pruned : 6848765, total: 11177025, Compression rate :       2.58x  ( 61.28% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7   0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0222, -0.0229, -0.0000, -0.0000, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0255,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0339, -0.0000, -0.0000,  0.0226],\n",
      "        [-0.0302, -0.0333, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0232,  0.0000,  0.0000, -0.0000,  0.0227],\n",
      "        [-0.0412,  0.0238, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0238, -0.0000,  0.0234, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0255,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0339, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0302, -0.0333, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0238, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0238, -0.0000,  0.0234, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:10/35]: ---\n",
      "model.conv1.weight   | nonzeros =    3278 /    9408 ( 34.84%) | total_pruned =    6130 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =      20 /      64 ( 31.25%) | total_pruned =      44 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =   12852 /   36864 ( 34.86%) | total_pruned =   24012 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =      20 /      64 ( 31.25%) | total_pruned =      44 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =   12852 /   36864 ( 34.86%) | total_pruned =   24012 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =      20 /      64 ( 31.25%) | total_pruned =      44 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =   12852 /   36864 ( 34.86%) | total_pruned =   24012 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =      20 /      64 ( 31.25%) | total_pruned =      44 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =   12852 /   36864 ( 34.86%) | total_pruned =   24012 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =      20 /      64 ( 31.25%) | total_pruned =      44 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =   25705 /   73728 ( 34.86%) | total_pruned =   48023 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =      41 /     128 ( 32.03%) | total_pruned =      87 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =   51412 /  147456 ( 34.87%) | total_pruned =   96044 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =      41 /     128 ( 32.03%) | total_pruned =      87 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =    2854 /    8192 ( 34.84%) | total_pruned =    5338 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =      41 /     128 ( 32.03%) | total_pruned =      87 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =   51412 /  147456 ( 34.87%) | total_pruned =   96044 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =      41 /     128 ( 32.03%) | total_pruned =      87 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =   51412 /  147456 ( 34.87%) | total_pruned =   96044 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =      41 /     128 ( 32.03%) | total_pruned =      87 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =  102827 /  294912 ( 34.87%) | total_pruned =  192085 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =      88 /     256 ( 34.38%) | total_pruned =     168 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =  205658 /  589824 ( 34.87%) | total_pruned =  384166 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =      88 /     256 ( 34.38%) | total_pruned =     168 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =   11423 /   32768 ( 34.86%) | total_pruned =   21345 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =      88 /     256 ( 34.38%) | total_pruned =     168 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =  205657 /  589824 ( 34.87%) | total_pruned =  384167 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =      88 /     256 ( 34.38%) | total_pruned =     168 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =  205657 /  589824 ( 34.87%) | total_pruned =  384167 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =      88 /     256 ( 34.38%) | total_pruned =     168 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =  411316 / 1179648 ( 34.87%) | total_pruned =  768332 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =     176 /     512 ( 34.38%) | total_pruned =     336 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros =  822634 / 2359296 ( 34.87%) | total_pruned = 1536662 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =     176 /     512 ( 34.38%) | total_pruned =     336 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =   45700 /  131072 ( 34.87%) | total_pruned =   85372 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =     176 /     512 ( 34.38%) | total_pruned =     336 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =  822634 / 2359296 ( 34.87%) | total_pruned = 1536662 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =     176 /     512 ( 34.38%) | total_pruned =     336 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =  822633 / 2359296 ( 34.87%) | total_pruned = 1536663 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =     176 /     512 ( 34.38%) | total_pruned =     336 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =     176 /     512 ( 34.38%) | total_pruned =     336 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 3895422, pruned : 7281603, total: 11177025, Compression rate :       2.87x  ( 65.15% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9   0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0255,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0339, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0302, -0.0333, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0238, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0238, -0.0000,  0.0234, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0255,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0339, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0302, -0.0333, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:11/35]: ---\n",
      "model.conv1.weight   | nonzeros =    2950 /    9408 ( 31.36%) | total_pruned =    6458 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =      18 /      64 ( 28.12%) | total_pruned =      46 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =   11566 /   36864 ( 31.37%) | total_pruned =   25298 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =      18 /      64 ( 28.12%) | total_pruned =      46 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =   11566 /   36864 ( 31.37%) | total_pruned =   25298 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =      18 /      64 ( 28.12%) | total_pruned =      46 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =   11566 /   36864 ( 31.37%) | total_pruned =   25298 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =      18 /      64 ( 28.12%) | total_pruned =      46 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =   11566 /   36864 ( 31.37%) | total_pruned =   25298 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =      18 /      64 ( 28.12%) | total_pruned =      46 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =   23134 /   73728 ( 31.38%) | total_pruned =   50594 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =      37 /     128 ( 28.91%) | total_pruned =      91 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =   46270 /  147456 ( 31.38%) | total_pruned =  101186 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =      37 /     128 ( 28.91%) | total_pruned =      91 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =    2568 /    8192 ( 31.35%) | total_pruned =    5624 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =      37 /     128 ( 28.91%) | total_pruned =      91 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =   46270 /  147456 ( 31.38%) | total_pruned =  101186 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =      37 /     128 ( 28.91%) | total_pruned =      91 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =   46270 /  147456 ( 31.38%) | total_pruned =  101186 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =      37 /     128 ( 28.91%) | total_pruned =      91 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =   92544 /  294912 ( 31.38%) | total_pruned =  202368 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =      79 /     256 ( 30.86%) | total_pruned =     177 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =  185092 /  589824 ( 31.38%) | total_pruned =  404732 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =      79 /     256 ( 30.86%) | total_pruned =     177 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =   10280 /   32768 ( 31.37%) | total_pruned =   22488 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =      79 /     256 ( 30.86%) | total_pruned =     177 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =  185091 /  589824 ( 31.38%) | total_pruned =  404733 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =      79 /     256 ( 30.86%) | total_pruned =     177 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =  185091 /  589824 ( 31.38%) | total_pruned =  404733 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =      79 /     256 ( 30.86%) | total_pruned =     177 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =  370184 / 1179648 ( 31.38%) | total_pruned =  809464 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =     158 /     512 ( 30.86%) | total_pruned =     354 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros =  740370 / 2359296 ( 31.38%) | total_pruned = 1618926 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =     158 /     512 ( 30.86%) | total_pruned =     354 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =   41130 /  131072 ( 31.38%) | total_pruned =   89942 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =     158 /     512 ( 30.86%) | total_pruned =     354 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =  740371 / 2359296 ( 31.38%) | total_pruned = 1618925 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =     158 /     512 ( 30.86%) | total_pruned =     354 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =  740369 / 2359296 ( 31.38%) | total_pruned = 1618927 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =     158 /     512 ( 30.86%) | total_pruned =     354 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =     158 /     512 ( 30.86%) | total_pruned =     354 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 3505867, pruned : 7671158, total: 11177025, Compression rate :       3.19x  ( 68.63% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9  31.4\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0255,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0339, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0302, -0.0333, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0339, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0302, -0.0333, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:12/35]: ---\n",
      "model.conv1.weight   | nonzeros =    2655 /    9408 ( 28.22%) | total_pruned =    6753 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =      16 /      64 ( 25.00%) | total_pruned =      48 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =   10409 /   36864 ( 28.24%) | total_pruned =   26455 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =      16 /      64 ( 25.00%) | total_pruned =      48 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =   10409 /   36864 ( 28.24%) | total_pruned =   26455 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =      16 /      64 ( 25.00%) | total_pruned =      48 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =   10409 /   36864 ( 28.24%) | total_pruned =   26455 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =      16 /      64 ( 25.00%) | total_pruned =      48 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =   10409 /   36864 ( 28.24%) | total_pruned =   26455 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =      16 /      64 ( 25.00%) | total_pruned =      48 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =   20820 /   73728 ( 28.24%) | total_pruned =   52908 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =      33 /     128 ( 25.78%) | total_pruned =      95 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =   41643 /  147456 ( 28.24%) | total_pruned =  105813 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =      33 /     128 ( 25.78%) | total_pruned =      95 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =    2311 /    8192 ( 28.21%) | total_pruned =    5881 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =      33 /     128 ( 25.78%) | total_pruned =      95 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =   41643 /  147456 ( 28.24%) | total_pruned =  105813 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =      33 /     128 ( 25.78%) | total_pruned =      95 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =   41643 /  147456 ( 28.24%) | total_pruned =  105813 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =      33 /     128 ( 25.78%) | total_pruned =      95 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =   83289 /  294912 ( 28.24%) | total_pruned =  211623 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =      71 /     256 ( 27.73%) | total_pruned =     185 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =  166582 /  589824 ( 28.24%) | total_pruned =  423242 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =      71 /     256 ( 27.73%) | total_pruned =     185 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =    9252 /   32768 ( 28.23%) | total_pruned =   23516 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =      71 /     256 ( 27.73%) | total_pruned =     185 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =  166582 /  589824 ( 28.24%) | total_pruned =  423242 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =      71 /     256 ( 27.73%) | total_pruned =     185 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =  166582 /  589824 ( 28.24%) | total_pruned =  423242 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =      71 /     256 ( 27.73%) | total_pruned =     185 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =  333165 / 1179648 ( 28.24%) | total_pruned =  846483 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =     142 /     512 ( 27.73%) | total_pruned =     370 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros =  666333 / 2359296 ( 28.24%) | total_pruned = 1692963 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =     142 /     512 ( 27.73%) | total_pruned =     370 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =   37017 /  131072 ( 28.24%) | total_pruned =   94055 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =     142 /     512 ( 27.73%) | total_pruned =     370 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =  666334 / 2359296 ( 28.24%) | total_pruned = 1692962 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =     142 /     512 ( 27.73%) | total_pruned =     370 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =  666332 / 2359296 ( 28.24%) | total_pruned = 1692964 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =     142 /     512 ( 27.73%) | total_pruned =     370 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =     142 /     512 ( 27.73%) | total_pruned =     370 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 3155272, pruned : 8021753, total: 11177025, Compression rate :       3.54x  ( 71.77% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9  31.4\n",
      "  28.2   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0269,  0.0318],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0339, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0302, -0.0333, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0318],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0339, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0302, -0.0333, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:13/35]: ---\n",
      "model.conv1.weight   | nonzeros =    2389 /    9408 ( 25.39%) | total_pruned =    7019 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =      14 /      64 ( 21.88%) | total_pruned =      50 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =    9368 /   36864 ( 25.41%) | total_pruned =   27496 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =      14 /      64 ( 21.88%) | total_pruned =      50 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =    9368 /   36864 ( 25.41%) | total_pruned =   27496 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =      14 /      64 ( 21.88%) | total_pruned =      50 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =    9368 /   36864 ( 25.41%) | total_pruned =   27496 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =      14 /      64 ( 21.88%) | total_pruned =      50 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =    9368 /   36864 ( 25.41%) | total_pruned =   27496 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =      14 /      64 ( 21.88%) | total_pruned =      50 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =   18738 /   73728 ( 25.42%) | total_pruned =   54990 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =      29 /     128 ( 22.66%) | total_pruned =      99 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =   37478 /  147456 ( 25.42%) | total_pruned =  109978 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =      29 /     128 ( 22.66%) | total_pruned =      99 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =    2080 /    8192 ( 25.39%) | total_pruned =    6112 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =      29 /     128 ( 22.66%) | total_pruned =      99 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =   37478 /  147456 ( 25.42%) | total_pruned =  109978 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =      29 /     128 ( 22.66%) | total_pruned =      99 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =   37478 /  147456 ( 25.42%) | total_pruned =  109978 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =      29 /     128 ( 22.66%) | total_pruned =      99 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =   74960 /  294912 ( 25.42%) | total_pruned =  219952 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =      64 /     256 ( 25.00%) | total_pruned =     192 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =  149923 /  589824 ( 25.42%) | total_pruned =  439901 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =      64 /     256 ( 25.00%) | total_pruned =     192 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =    8326 /   32768 ( 25.41%) | total_pruned =   24442 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =      64 /     256 ( 25.00%) | total_pruned =     192 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =  149923 /  589824 ( 25.42%) | total_pruned =  439901 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =      64 /     256 ( 25.00%) | total_pruned =     192 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =  149923 /  589824 ( 25.42%) | total_pruned =  439901 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =      64 /     256 ( 25.00%) | total_pruned =     192 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =  299848 / 1179648 ( 25.42%) | total_pruned =  879800 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =     127 /     512 ( 24.80%) | total_pruned =     385 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros =  599701 / 2359296 ( 25.42%) | total_pruned = 1759595 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =     127 /     512 ( 24.80%) | total_pruned =     385 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =   33315 /  131072 ( 25.42%) | total_pruned =   97757 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =     127 /     512 ( 24.80%) | total_pruned =     385 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =  599700 / 2359296 ( 25.42%) | total_pruned = 1759596 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =     127 /     512 ( 24.80%) | total_pruned =     385 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =  599699 / 2359296 ( 25.42%) | total_pruned = 1759597 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =     127 /     512 ( 24.80%) | total_pruned =     385 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =     127 /     512 ( 24.80%) | total_pruned =     385 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 2839729, pruned : 8337296, total: 11177025, Compression rate :       3.94x  ( 74.59% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9  31.4\n",
      "  28.2  25.4   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0318],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0339, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0302, -0.0333, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0318],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0339, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0302, -0.0333, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:14/35]: ---\n",
      "model.conv1.weight   | nonzeros =    2150 /    9408 ( 22.85%) | total_pruned =    7258 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =      12 /      64 ( 18.75%) | total_pruned =      52 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =    8431 /   36864 ( 22.87%) | total_pruned =   28433 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =      12 /      64 ( 18.75%) | total_pruned =      52 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =    8431 /   36864 ( 22.87%) | total_pruned =   28433 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =      12 /      64 ( 18.75%) | total_pruned =      52 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =    8431 /   36864 ( 22.87%) | total_pruned =   28433 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =      12 /      64 ( 18.75%) | total_pruned =      52 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =    8431 /   36864 ( 22.87%) | total_pruned =   28433 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =      12 /      64 ( 18.75%) | total_pruned =      52 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =   16864 /   73728 ( 22.87%) | total_pruned =   56864 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =      26 /     128 ( 20.31%) | total_pruned =     102 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =   33730 /  147456 ( 22.87%) | total_pruned =  113726 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =      26 /     128 ( 20.31%) | total_pruned =     102 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =    1872 /    8192 ( 22.85%) | total_pruned =    6320 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =      26 /     128 ( 20.31%) | total_pruned =     102 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =   33730 /  147456 ( 22.87%) | total_pruned =  113726 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =      26 /     128 ( 20.31%) | total_pruned =     102 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =   33730 /  147456 ( 22.87%) | total_pruned =  113726 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =      26 /     128 ( 20.31%) | total_pruned =     102 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =   67464 /  294912 ( 22.88%) | total_pruned =  227448 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =      57 /     256 ( 22.27%) | total_pruned =     199 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =  134930 /  589824 ( 22.88%) | total_pruned =  454894 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =      57 /     256 ( 22.27%) | total_pruned =     199 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =    7493 /   32768 ( 22.87%) | total_pruned =   25275 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =      57 /     256 ( 22.27%) | total_pruned =     199 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =  134930 /  589824 ( 22.88%) | total_pruned =  454894 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =      57 /     256 ( 22.27%) | total_pruned =     199 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =  134930 /  589824 ( 22.88%) | total_pruned =  454894 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =      57 /     256 ( 22.27%) | total_pruned =     199 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =  269863 / 1179648 ( 22.88%) | total_pruned =  909785 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =     114 /     512 ( 22.27%) | total_pruned =     398 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros =  539731 / 2359296 ( 22.88%) | total_pruned = 1819565 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =     114 /     512 ( 22.27%) | total_pruned =     398 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =   29983 /  131072 ( 22.88%) | total_pruned =  101089 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =     114 /     512 ( 22.27%) | total_pruned =     398 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =  539730 / 2359296 ( 22.88%) | total_pruned = 1819566 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =     114 /     512 ( 22.27%) | total_pruned =     398 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =  539729 / 2359296 ( 22.88%) | total_pruned = 1819567 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =     114 /     512 ( 22.27%) | total_pruned =     398 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =     114 /     512 ( 22.27%) | total_pruned =     398 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 2555743, pruned : 8621282, total: 11177025, Compression rate :       4.37x  ( 77.13% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9  31.4\n",
      "  28.2  25.4  22.9   0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0318],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0339, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0302, -0.0333, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0318],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0339, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0333, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:15/35]: ---\n",
      "model.conv1.weight   | nonzeros =    1935 /    9408 ( 20.57%) | total_pruned =    7473 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =      10 /      64 ( 15.62%) | total_pruned =      54 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =    7588 /   36864 ( 20.58%) | total_pruned =   29276 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =      10 /      64 ( 15.62%) | total_pruned =      54 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =    7588 /   36864 ( 20.58%) | total_pruned =   29276 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =      10 /      64 ( 15.62%) | total_pruned =      54 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =    7588 /   36864 ( 20.58%) | total_pruned =   29276 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =      10 /      64 ( 15.62%) | total_pruned =      54 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =    7588 /   36864 ( 20.58%) | total_pruned =   29276 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =      10 /      64 ( 15.62%) | total_pruned =      54 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =   15177 /   73728 ( 20.59%) | total_pruned =   58551 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =      23 /     128 ( 17.97%) | total_pruned =     105 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =   30357 /  147456 ( 20.59%) | total_pruned =  117099 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =      23 /     128 ( 17.97%) | total_pruned =     105 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =    1684 /    8192 ( 20.56%) | total_pruned =    6508 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =      23 /     128 ( 17.97%) | total_pruned =     105 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =   30357 /  147456 ( 20.59%) | total_pruned =  117099 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =      23 /     128 ( 17.97%) | total_pruned =     105 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =   30357 /  147456 ( 20.59%) | total_pruned =  117099 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =      23 /     128 ( 17.97%) | total_pruned =     105 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =   60717 /  294912 ( 20.59%) | total_pruned =  234195 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =      51 /     256 ( 19.92%) | total_pruned =     205 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =  121437 /  589824 ( 20.59%) | total_pruned =  468387 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =      51 /     256 ( 19.92%) | total_pruned =     205 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =    6743 /   32768 ( 20.58%) | total_pruned =   26025 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =      51 /     256 ( 19.92%) | total_pruned =     205 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =  121437 /  589824 ( 20.59%) | total_pruned =  468387 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =      51 /     256 ( 19.92%) | total_pruned =     205 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =  121437 /  589824 ( 20.59%) | total_pruned =  468387 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =      51 /     256 ( 19.92%) | total_pruned =     205 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =  242877 / 1179648 ( 20.59%) | total_pruned =  936771 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =     102 /     512 ( 19.92%) | total_pruned =     410 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros =  485758 / 2359296 ( 20.59%) | total_pruned = 1873538 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =     102 /     512 ( 19.92%) | total_pruned =     410 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =   26984 /  131072 ( 20.59%) | total_pruned =  104088 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =     102 /     512 ( 19.92%) | total_pruned =     410 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =  485757 / 2359296 ( 20.59%) | total_pruned = 1873539 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =     102 /     512 ( 19.92%) | total_pruned =     410 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =  485756 / 2359296 ( 20.59%) | total_pruned = 1873540 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =     102 /     512 ( 19.92%) | total_pruned =     410 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =     102 /     512 ( 19.92%) | total_pruned =     410 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 2300155, pruned : 8876870, total: 11177025, Compression rate :       4.86x  ( 79.42% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9  31.4\n",
      "  28.2  25.4  22.9  20.6   0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0318],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0339, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0333, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0339, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0333, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:16/35]: ---\n",
      "model.conv1.weight   | nonzeros =    1741 /    9408 ( 18.51%) | total_pruned =    7667 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =       9 /      64 ( 14.06%) | total_pruned =      55 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =    6829 /   36864 ( 18.52%) | total_pruned =   30035 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =       9 /      64 ( 14.06%) | total_pruned =      55 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =    6829 /   36864 ( 18.52%) | total_pruned =   30035 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =       9 /      64 ( 14.06%) | total_pruned =      55 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =    6829 /   36864 ( 18.52%) | total_pruned =   30035 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =       9 /      64 ( 14.06%) | total_pruned =      55 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =    6829 /   36864 ( 18.52%) | total_pruned =   30035 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =       9 /      64 ( 14.06%) | total_pruned =      55 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =   13659 /   73728 ( 18.53%) | total_pruned =   60069 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =      20 /     128 ( 15.62%) | total_pruned =     108 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =   27321 /  147456 ( 18.53%) | total_pruned =  120135 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =      20 /     128 ( 15.62%) | total_pruned =     108 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =    1515 /    8192 ( 18.49%) | total_pruned =    6677 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =      20 /     128 ( 15.62%) | total_pruned =     108 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =   27321 /  147456 ( 18.53%) | total_pruned =  120135 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =      20 /     128 ( 15.62%) | total_pruned =     108 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =   27321 /  147456 ( 18.53%) | total_pruned =  120135 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =      20 /     128 ( 15.62%) | total_pruned =     108 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =   54645 /  294912 ( 18.53%) | total_pruned =  240267 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =      46 /     256 ( 17.97%) | total_pruned =     210 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =  109293 /  589824 ( 18.53%) | total_pruned =  480531 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =      46 /     256 ( 17.97%) | total_pruned =     210 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =    6068 /   32768 ( 18.52%) | total_pruned =   26700 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =      46 /     256 ( 17.97%) | total_pruned =     210 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =  109293 /  589824 ( 18.53%) | total_pruned =  480531 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =      46 /     256 ( 17.97%) | total_pruned =     210 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =  109293 /  589824 ( 18.53%) | total_pruned =  480531 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =      46 /     256 ( 17.97%) | total_pruned =     210 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =  218589 / 1179648 ( 18.53%) | total_pruned =  961059 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =      91 /     512 ( 17.77%) | total_pruned =     421 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros =  437182 / 2359296 ( 18.53%) | total_pruned = 1922114 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =      91 /     512 ( 17.77%) | total_pruned =     421 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =   24285 /  131072 ( 18.53%) | total_pruned =  106787 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =      91 /     512 ( 17.77%) | total_pruned =     421 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =  437181 / 2359296 ( 18.53%) | total_pruned = 1922115 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =      91 /     512 ( 17.77%) | total_pruned =     421 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =  437181 / 2359296 ( 18.53%) | total_pruned = 1922115 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =      91 /     512 ( 17.77%) | total_pruned =     421 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =      91 /     512 ( 17.77%) | total_pruned =     421 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 2070126, pruned : 9106899, total: 11177025, Compression rate :       5.40x  ( 81.48% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9  31.4\n",
      "  28.2  25.4  22.9  20.6  18.5   0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0339, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0333, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:17/35]: ---\n",
      "model.conv1.weight   | nonzeros =    1567 /    9408 ( 16.66%) | total_pruned =    7841 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =       8 /      64 ( 12.50%) | total_pruned =      56 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =    6146 /   36864 ( 16.67%) | total_pruned =   30718 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =       8 /      64 ( 12.50%) | total_pruned =      56 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =    6146 /   36864 ( 16.67%) | total_pruned =   30718 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =       8 /      64 ( 12.50%) | total_pruned =      56 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =    6146 /   36864 ( 16.67%) | total_pruned =   30718 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =       8 /      64 ( 12.50%) | total_pruned =      56 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =    6146 /   36864 ( 16.67%) | total_pruned =   30718 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =       8 /      64 ( 12.50%) | total_pruned =      56 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =   12293 /   73728 ( 16.67%) | total_pruned =   61435 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =      18 /     128 ( 14.06%) | total_pruned =     110 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =   24589 /  147456 ( 16.68%) | total_pruned =  122867 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =      18 /     128 ( 14.06%) | total_pruned =     110 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =    1363 /    8192 ( 16.64%) | total_pruned =    6829 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =      18 /     128 ( 14.06%) | total_pruned =     110 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =   24589 /  147456 ( 16.68%) | total_pruned =  122867 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =      18 /     128 ( 14.06%) | total_pruned =     110 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =   24589 /  147456 ( 16.68%) | total_pruned =  122867 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =      18 /     128 ( 14.06%) | total_pruned =     110 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =   49180 /  294912 ( 16.68%) | total_pruned =  245732 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =      41 /     256 ( 16.02%) | total_pruned =     215 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =   98363 /  589824 ( 16.68%) | total_pruned =  491461 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =      41 /     256 ( 16.02%) | total_pruned =     215 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =    5461 /   32768 ( 16.67%) | total_pruned =   27307 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =      41 /     256 ( 16.02%) | total_pruned =     215 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =   98363 /  589824 ( 16.68%) | total_pruned =  491461 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =      41 /     256 ( 16.02%) | total_pruned =     215 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =   98363 /  589824 ( 16.68%) | total_pruned =  491461 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =      41 /     256 ( 16.02%) | total_pruned =     215 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =  196730 / 1179648 ( 16.68%) | total_pruned =  982918 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =      82 /     512 ( 16.02%) | total_pruned =     430 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros =  393463 / 2359296 ( 16.68%) | total_pruned = 1965833 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =      82 /     512 ( 16.02%) | total_pruned =     430 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =   21856 /  131072 ( 16.67%) | total_pruned =  109216 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =      82 /     512 ( 16.02%) | total_pruned =     430 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =  393463 / 2359296 ( 16.68%) | total_pruned = 1965833 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =      82 /     512 ( 16.02%) | total_pruned =     430 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =  393463 / 2359296 ( 16.68%) | total_pruned = 1965833 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =      82 /     512 ( 16.02%) | total_pruned =     430 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =      82 /     512 ( 16.02%) | total_pruned =     430 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 1863107, pruned : 9313918, total: 11177025, Compression rate :       6.00x  ( 83.33% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9  31.4\n",
      "  28.2  25.4  22.9  20.6  18.5  16.7   0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:18/35]: ---\n",
      "model.conv1.weight   | nonzeros =    1410 /    9408 ( 14.99%) | total_pruned =    7998 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =       7 /      64 ( 10.94%) | total_pruned =      57 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =    5531 /   36864 ( 15.00%) | total_pruned =   31333 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =       7 /      64 ( 10.94%) | total_pruned =      57 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =    5531 /   36864 ( 15.00%) | total_pruned =   31333 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =       7 /      64 ( 10.94%) | total_pruned =      57 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =    5531 /   36864 ( 15.00%) | total_pruned =   31333 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =       7 /      64 ( 10.94%) | total_pruned =      57 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =    5531 /   36864 ( 15.00%) | total_pruned =   31333 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =       7 /      64 ( 10.94%) | total_pruned =      57 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =   11063 /   73728 ( 15.01%) | total_pruned =   62665 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =      16 /     128 ( 12.50%) | total_pruned =     112 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =   22130 /  147456 ( 15.01%) | total_pruned =  125326 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =      16 /     128 ( 12.50%) | total_pruned =     112 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =    1226 /    8192 ( 14.97%) | total_pruned =    6966 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =      16 /     128 ( 12.50%) | total_pruned =     112 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =   22130 /  147456 ( 15.01%) | total_pruned =  125326 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =      16 /     128 ( 12.50%) | total_pruned =     112 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =   22130 /  147456 ( 15.01%) | total_pruned =  125326 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =      16 /     128 ( 12.50%) | total_pruned =     112 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =   44262 /  294912 ( 15.01%) | total_pruned =  250650 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =      37 /     256 ( 14.45%) | total_pruned =     219 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =   88526 /  589824 ( 15.01%) | total_pruned =  501298 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =      37 /     256 ( 14.45%) | total_pruned =     219 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =    4915 /   32768 ( 15.00%) | total_pruned =   27853 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =      37 /     256 ( 14.45%) | total_pruned =     219 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =   88526 /  589824 ( 15.01%) | total_pruned =  501298 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =      37 /     256 ( 14.45%) | total_pruned =     219 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =   88526 /  589824 ( 15.01%) | total_pruned =  501298 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =      37 /     256 ( 14.45%) | total_pruned =     219 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =  177057 / 1179648 ( 15.01%) | total_pruned = 1002591 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =      73 /     512 ( 14.26%) | total_pruned =     439 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros =  354116 / 2359296 ( 15.01%) | total_pruned = 2005180 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =      73 /     512 ( 14.26%) | total_pruned =     439 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =   19670 /  131072 ( 15.01%) | total_pruned =  111402 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =      73 /     512 ( 14.26%) | total_pruned =     439 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =  354116 / 2359296 ( 15.01%) | total_pruned = 2005180 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =      73 /     512 ( 14.26%) | total_pruned =     439 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =  354116 / 2359296 ( 15.01%) | total_pruned = 2005180 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =      73 /     512 ( 14.26%) | total_pruned =     439 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =      73 /     512 ( 14.26%) | total_pruned =     439 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 1676782, pruned : 9500243, total: 11177025, Compression rate :       6.67x  ( 85.00% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9  31.4\n",
      "  28.2  25.4  22.9  20.6  18.5  16.7  15.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:19/35]: ---\n",
      "model.conv1.weight   | nonzeros =    1269 /    9408 ( 13.49%) | total_pruned =    8139 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =       6 /      64 (  9.38%) | total_pruned =      58 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =    4978 /   36864 ( 13.50%) | total_pruned =   31886 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =       6 /      64 (  9.38%) | total_pruned =      58 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =    4978 /   36864 ( 13.50%) | total_pruned =   31886 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =       6 /      64 (  9.38%) | total_pruned =      58 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =    4978 /   36864 ( 13.50%) | total_pruned =   31886 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =       6 /      64 (  9.38%) | total_pruned =      58 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =    4978 /   36864 ( 13.50%) | total_pruned =   31886 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =       6 /      64 (  9.38%) | total_pruned =      58 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =    9956 /   73728 ( 13.50%) | total_pruned =   63772 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =      14 /     128 ( 10.94%) | total_pruned =     114 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =   19917 /  147456 ( 13.51%) | total_pruned =  127539 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =      14 /     128 ( 10.94%) | total_pruned =     114 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =    1103 /    8192 ( 13.46%) | total_pruned =    7089 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =      14 /     128 ( 10.94%) | total_pruned =     114 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =   19917 /  147456 ( 13.51%) | total_pruned =  127539 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =      14 /     128 ( 10.94%) | total_pruned =     114 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =   19917 /  147456 ( 13.51%) | total_pruned =  127539 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =      14 /     128 ( 10.94%) | total_pruned =     114 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =   39835 /  294912 ( 13.51%) | total_pruned =  255077 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =      33 /     256 ( 12.89%) | total_pruned =     223 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =   79673 /  589824 ( 13.51%) | total_pruned =  510151 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =      33 /     256 ( 12.89%) | total_pruned =     223 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =    4423 /   32768 ( 13.50%) | total_pruned =   28345 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =      33 /     256 ( 12.89%) | total_pruned =     223 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =   79673 /  589824 ( 13.51%) | total_pruned =  510151 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =      33 /     256 ( 12.89%) | total_pruned =     223 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =   79673 /  589824 ( 13.51%) | total_pruned =  510151 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =      33 /     256 ( 12.89%) | total_pruned =     223 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =  159351 / 1179648 ( 13.51%) | total_pruned = 1020297 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =      65 /     512 ( 12.70%) | total_pruned =     447 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros =  318704 / 2359296 ( 13.51%) | total_pruned = 2040592 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =      65 /     512 ( 12.70%) | total_pruned =     447 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =   17703 /  131072 ( 13.51%) | total_pruned =  113369 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =      65 /     512 ( 12.70%) | total_pruned =     447 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =  318704 / 2359296 ( 13.51%) | total_pruned = 2040592 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =      65 /     512 ( 12.70%) | total_pruned =     447 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =  318705 / 2359296 ( 13.51%) | total_pruned = 2040591 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =      65 /     512 ( 12.70%) | total_pruned =     447 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =      65 /     512 ( 12.70%) | total_pruned =     447 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 1509091, pruned : 9667934, total: 11177025, Compression rate :       7.41x  ( 86.50% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9  31.4\n",
      "  28.2  25.4  22.9  20.6  18.5  16.7  15.   13.5   0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:20/35]: ---\n",
      "model.conv1.weight   | nonzeros =    1142 /    9408 ( 12.14%) | total_pruned =    8266 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =       5 /      64 (  7.81%) | total_pruned =      59 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =    4480 /   36864 ( 12.15%) | total_pruned =   32384 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =       5 /      64 (  7.81%) | total_pruned =      59 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =    4480 /   36864 ( 12.15%) | total_pruned =   32384 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =       5 /      64 (  7.81%) | total_pruned =      59 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =    4480 /   36864 ( 12.15%) | total_pruned =   32384 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =       5 /      64 (  7.81%) | total_pruned =      59 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =    4480 /   36864 ( 12.15%) | total_pruned =   32384 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =       5 /      64 (  7.81%) | total_pruned =      59 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =    8960 /   73728 ( 12.15%) | total_pruned =   64768 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =      12 /     128 (  9.38%) | total_pruned =     116 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =   17925 /  147456 ( 12.16%) | total_pruned =  129531 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =      12 /     128 (  9.38%) | total_pruned =     116 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =     992 /    8192 ( 12.11%) | total_pruned =    7200 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =      12 /     128 (  9.38%) | total_pruned =     116 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =   17925 /  147456 ( 12.16%) | total_pruned =  129531 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =      12 /     128 (  9.38%) | total_pruned =     116 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =   17925 /  147456 ( 12.16%) | total_pruned =  129531 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =      12 /     128 (  9.38%) | total_pruned =     116 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =   35851 /  294912 ( 12.16%) | total_pruned =  259061 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =      29 /     256 ( 11.33%) | total_pruned =     227 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =   71705 /  589824 ( 12.16%) | total_pruned =  518119 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =      29 /     256 ( 11.33%) | total_pruned =     227 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =    3980 /   32768 ( 12.15%) | total_pruned =   28788 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =      29 /     256 ( 11.33%) | total_pruned =     227 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =   71705 /  589824 ( 12.16%) | total_pruned =  518119 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =      29 /     256 ( 11.33%) | total_pruned =     227 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =   71706 /  589824 ( 12.16%) | total_pruned =  518118 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =      29 /     256 ( 11.33%) | total_pruned =     227 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =  143416 / 1179648 ( 12.16%) | total_pruned = 1036232 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =      58 /     512 ( 11.33%) | total_pruned =     454 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros =  286833 / 2359296 ( 12.16%) | total_pruned = 2072463 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =      58 /     512 ( 11.33%) | total_pruned =     454 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =   15932 /  131072 ( 12.16%) | total_pruned =  115140 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =      58 /     512 ( 11.33%) | total_pruned =     454 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =  286833 / 2359296 ( 12.16%) | total_pruned = 2072463 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =      58 /     512 ( 11.33%) | total_pruned =     454 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =  286835 / 2359296 ( 12.16%) | total_pruned = 2072461 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =      58 /     512 ( 11.33%) | total_pruned =     454 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =      58 /     512 ( 11.33%) | total_pruned =     454 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 1358164, pruned : 9818861, total: 11177025, Compression rate :       8.23x  ( 87.85% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9  31.4\n",
      "  28.2  25.4  22.9  20.6  18.5  16.7  15.   13.5  12.2   0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:21/35]: ---\n",
      "model.conv1.weight   | nonzeros =    1027 /    9408 ( 10.92%) | total_pruned =    8381 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =       4 /      64 (  6.25%) | total_pruned =      60 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =    4032 /   36864 ( 10.94%) | total_pruned =   32832 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =       4 /      64 (  6.25%) | total_pruned =      60 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =    4032 /   36864 ( 10.94%) | total_pruned =   32832 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =       4 /      64 (  6.25%) | total_pruned =      60 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =    4032 /   36864 ( 10.94%) | total_pruned =   32832 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =       4 /      64 (  6.25%) | total_pruned =      60 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =    4032 /   36864 ( 10.94%) | total_pruned =   32832 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =       4 /      64 (  6.25%) | total_pruned =      60 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =    8064 /   73728 ( 10.94%) | total_pruned =   65664 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =      10 /     128 (  7.81%) | total_pruned =     118 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =   16132 /  147456 ( 10.94%) | total_pruned =  131324 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =      10 /     128 (  7.81%) | total_pruned =     118 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =     892 /    8192 ( 10.89%) | total_pruned =    7300 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =      10 /     128 (  7.81%) | total_pruned =     118 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =   16132 /  147456 ( 10.94%) | total_pruned =  131324 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =      10 /     128 (  7.81%) | total_pruned =     118 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =   16132 /  147456 ( 10.94%) | total_pruned =  131324 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =      10 /     128 (  7.81%) | total_pruned =     118 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =   32266 /  294912 ( 10.94%) | total_pruned =  262646 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =      26 /     256 ( 10.16%) | total_pruned =     230 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =   64534 /  589824 ( 10.94%) | total_pruned =  525290 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =      26 /     256 ( 10.16%) | total_pruned =     230 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =    3582 /   32768 ( 10.93%) | total_pruned =   29186 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =      26 /     256 ( 10.16%) | total_pruned =     230 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =   64534 /  589824 ( 10.94%) | total_pruned =  525290 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =      26 /     256 ( 10.16%) | total_pruned =     230 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =   64535 /  589824 ( 10.94%) | total_pruned =  525289 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =      26 /     256 ( 10.16%) | total_pruned =     230 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =  129074 / 1179648 ( 10.94%) | total_pruned = 1050574 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =      52 /     512 ( 10.16%) | total_pruned =     460 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros =  258149 / 2359296 ( 10.94%) | total_pruned = 2101147 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =      52 /     512 ( 10.16%) | total_pruned =     460 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =   14338 /  131072 ( 10.94%) | total_pruned =  116734 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =      52 /     512 ( 10.16%) | total_pruned =     460 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =  258150 / 2359296 ( 10.94%) | total_pruned = 2101146 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =      52 /     512 ( 10.16%) | total_pruned =     460 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =  258151 / 2359296 ( 10.94%) | total_pruned = 2101145 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =      52 /     512 ( 10.16%) | total_pruned =     460 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =      52 /     512 ( 10.16%) | total_pruned =     460 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 1222333, pruned : 9954692, total: 11177025, Compression rate :       9.14x  ( 89.06% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9  31.4\n",
      "  28.2  25.4  22.9  20.6  18.5  16.7  15.   13.5  12.2  10.9   0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:22/35]: ---\n",
      "model.conv1.weight   | nonzeros =     924 /    9408 (  9.82%) | total_pruned =    8484 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =       3 /      64 (  4.69%) | total_pruned =      61 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =    3628 /   36864 (  9.84%) | total_pruned =   33236 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =       3 /      64 (  4.69%) | total_pruned =      61 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =    3628 /   36864 (  9.84%) | total_pruned =   33236 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =       3 /      64 (  4.69%) | total_pruned =      61 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =    3628 /   36864 (  9.84%) | total_pruned =   33236 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =       3 /      64 (  4.69%) | total_pruned =      61 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =    3628 /   36864 (  9.84%) | total_pruned =   33236 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =       3 /      64 (  4.69%) | total_pruned =      61 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =    7257 /   73728 (  9.84%) | total_pruned =   66471 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =       9 /     128 (  7.03%) | total_pruned =     119 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =   14518 /  147456 (  9.85%) | total_pruned =  132938 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =       9 /     128 (  7.03%) | total_pruned =     119 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =     802 /    8192 (  9.79%) | total_pruned =    7390 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =       9 /     128 (  7.03%) | total_pruned =     119 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =   14518 /  147456 (  9.85%) | total_pruned =  132938 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =       9 /     128 (  7.03%) | total_pruned =     119 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =   14518 /  147456 (  9.85%) | total_pruned =  132938 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =       9 /     128 (  7.03%) | total_pruned =     119 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =   29039 /  294912 (  9.85%) | total_pruned =  265873 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =      23 /     256 (  8.98%) | total_pruned =     233 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =   58080 /  589824 (  9.85%) | total_pruned =  531744 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =      23 /     256 (  8.98%) | total_pruned =     233 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =    3223 /   32768 (  9.84%) | total_pruned =   29545 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =      23 /     256 (  8.98%) | total_pruned =     233 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =   58080 /  589824 (  9.85%) | total_pruned =  531744 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =      23 /     256 (  8.98%) | total_pruned =     233 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =   58081 /  589824 (  9.85%) | total_pruned =  531743 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =      23 /     256 (  8.98%) | total_pruned =     233 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =  116166 / 1179648 (  9.85%) | total_pruned = 1063482 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =      46 /     512 (  8.98%) | total_pruned =     466 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros =  232334 / 2359296 (  9.85%) | total_pruned = 2126962 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =      46 /     512 (  8.98%) | total_pruned =     466 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =   12904 /  131072 (  9.84%) | total_pruned =  118168 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =      46 /     512 (  8.98%) | total_pruned =     466 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =  232335 / 2359296 (  9.85%) | total_pruned = 2126961 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =      46 /     512 (  8.98%) | total_pruned =     466 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =  232336 / 2359296 (  9.85%) | total_pruned = 2126960 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =      46 /     512 (  8.98%) | total_pruned =     466 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =      46 /     512 (  8.98%) | total_pruned =     466 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 1100079, pruned : 10076946, total: 11177025, Compression rate :      10.16x  ( 90.16% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9  31.4\n",
      "  28.2  25.4  22.9  20.6  18.5  16.7  15.   13.5  12.2  10.9   9.8   0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0412,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0416],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:23/35]: ---\n",
      "model.conv1.weight   | nonzeros =     831 /    9408 (  8.83%) | total_pruned =    8577 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =       2 /      64 (  3.12%) | total_pruned =      62 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =    3265 /   36864 (  8.86%) | total_pruned =   33599 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =       2 /      64 (  3.12%) | total_pruned =      62 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =    3265 /   36864 (  8.86%) | total_pruned =   33599 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =       2 /      64 (  3.12%) | total_pruned =      62 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =    3265 /   36864 (  8.86%) | total_pruned =   33599 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =       2 /      64 (  3.12%) | total_pruned =      62 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =    3265 /   36864 (  8.86%) | total_pruned =   33599 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =       2 /      64 (  3.12%) | total_pruned =      62 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =    6531 /   73728 (  8.86%) | total_pruned =   67197 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =       8 /     128 (  6.25%) | total_pruned =     120 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =   13066 /  147456 (  8.86%) | total_pruned =  134390 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =       8 /     128 (  6.25%) | total_pruned =     120 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =     721 /    8192 (  8.80%) | total_pruned =    7471 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =       8 /     128 (  6.25%) | total_pruned =     120 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =   13066 /  147456 (  8.86%) | total_pruned =  134390 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =       8 /     128 (  6.25%) | total_pruned =     120 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =   13066 /  147456 (  8.86%) | total_pruned =  134390 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =       8 /     128 (  6.25%) | total_pruned =     120 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =   26135 /  294912 (  8.86%) | total_pruned =  268777 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =      20 /     256 (  7.81%) | total_pruned =     236 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =   52272 /  589824 (  8.86%) | total_pruned =  537552 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =      20 /     256 (  7.81%) | total_pruned =     236 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =    2900 /   32768 (  8.85%) | total_pruned =   29868 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =      20 /     256 (  7.81%) | total_pruned =     236 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =   52272 /  589824 (  8.86%) | total_pruned =  537552 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =      20 /     256 (  7.81%) | total_pruned =     236 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =   52274 /  589824 (  8.86%) | total_pruned =  537550 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =      20 /     256 (  7.81%) | total_pruned =     236 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =  104549 / 1179648 (  8.86%) | total_pruned = 1075099 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =      41 /     512 (  8.01%) | total_pruned =     471 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros =  209100 / 2359296 (  8.86%) | total_pruned = 2150196 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =      41 /     512 (  8.01%) | total_pruned =     471 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =   11613 /  131072 (  8.86%) | total_pruned =  119459 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =      41 /     512 (  8.01%) | total_pruned =     471 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =  209101 / 2359296 (  8.86%) | total_pruned = 2150195 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =      41 /     512 (  8.01%) | total_pruned =     471 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =  209102 / 2359296 (  8.86%) | total_pruned = 2150194 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =      41 /     512 (  8.01%) | total_pruned =     471 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =      41 /     512 (  8.01%) | total_pruned =     471 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 990056, pruned : 10186969, total: 11177025, Compression rate :      11.29x  ( 91.14% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9  31.4\n",
      "  28.2  25.4  22.9  20.6  18.5  16.7  15.   13.5  12.2  10.9   9.8   8.9\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:24/35]: ---\n",
      "model.conv1.weight   | nonzeros =     748 /    9408 (  7.95%) | total_pruned =    8660 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =    2938 /   36864 (  7.97%) | total_pruned =   33926 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =    2938 /   36864 (  7.97%) | total_pruned =   33926 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =    2938 /   36864 (  7.97%) | total_pruned =   33926 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =    2938 /   36864 (  7.97%) | total_pruned =   33926 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =    5878 /   73728 (  7.97%) | total_pruned =   67850 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =       7 /     128 (  5.47%) | total_pruned =     121 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =   11759 /  147456 (  7.97%) | total_pruned =  135697 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =       7 /     128 (  5.47%) | total_pruned =     121 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =     649 /    8192 (  7.92%) | total_pruned =    7543 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =       7 /     128 (  5.47%) | total_pruned =     121 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =   11759 /  147456 (  7.97%) | total_pruned =  135697 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =       7 /     128 (  5.47%) | total_pruned =     121 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =   11759 /  147456 (  7.97%) | total_pruned =  135697 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =       7 /     128 (  5.47%) | total_pruned =     121 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =   23521 /  294912 (  7.98%) | total_pruned =  271391 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =      18 /     256 (  7.03%) | total_pruned =     238 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =   47044 /  589824 (  7.98%) | total_pruned =  542780 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =      18 /     256 (  7.03%) | total_pruned =     238 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =    2610 /   32768 (  7.97%) | total_pruned =   30158 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =      18 /     256 (  7.03%) | total_pruned =     238 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =   47044 /  589824 (  7.98%) | total_pruned =  542780 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =      18 /     256 (  7.03%) | total_pruned =     238 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =   47046 /  589824 (  7.98%) | total_pruned =  542778 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =      18 /     256 (  7.03%) | total_pruned =     238 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =   94094 / 1179648 (  7.98%) | total_pruned = 1085554 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =      37 /     512 (  7.23%) | total_pruned =     475 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros =  188190 / 2359296 (  7.98%) | total_pruned = 2171106 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =      37 /     512 (  7.23%) | total_pruned =     475 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =   10451 /  131072 (  7.97%) | total_pruned =  120621 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =      37 /     512 (  7.23%) | total_pruned =     475 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =  188191 / 2359296 (  7.98%) | total_pruned = 2171105 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =      37 /     512 (  7.23%) | total_pruned =     475 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =  188191 / 2359296 (  7.98%) | total_pruned = 2171105 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =      37 /     512 (  7.23%) | total_pruned =     475 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =      37 /     512 (  7.23%) | total_pruned =     475 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 891039, pruned : 10285986, total: 11177025, Compression rate :      12.54x  ( 92.03% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9  31.4\n",
      "  28.2  25.4  22.9  20.6  18.5  16.7  15.   13.5  12.2  10.9   9.8   8.9\n",
      "   8.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "25\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:25/35]: ---\n",
      "model.conv1.weight   | nonzeros =     673 /    9408 (  7.15%) | total_pruned =    8735 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =    2644 /   36864 (  7.17%) | total_pruned =   34220 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =    2644 /   36864 (  7.17%) | total_pruned =   34220 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =    2644 /   36864 (  7.17%) | total_pruned =   34220 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =    2644 /   36864 (  7.17%) | total_pruned =   34220 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =    5290 /   73728 (  7.18%) | total_pruned =   68438 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =       6 /     128 (  4.69%) | total_pruned =     122 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =   10583 /  147456 (  7.18%) | total_pruned =  136873 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =       6 /     128 (  4.69%) | total_pruned =     122 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =     584 /    8192 (  7.13%) | total_pruned =    7608 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =       6 /     128 (  4.69%) | total_pruned =     122 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =   10583 /  147456 (  7.18%) | total_pruned =  136873 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =       6 /     128 (  4.69%) | total_pruned =     122 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =   10583 /  147456 (  7.18%) | total_pruned =  136873 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =       6 /     128 (  4.69%) | total_pruned =     122 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =   21169 /  294912 (  7.18%) | total_pruned =  273743 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =      16 /     256 (  6.25%) | total_pruned =     240 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =   42339 /  589824 (  7.18%) | total_pruned =  547485 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =      16 /     256 (  6.25%) | total_pruned =     240 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =    2349 /   32768 (  7.17%) | total_pruned =   30419 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =      16 /     256 (  6.25%) | total_pruned =     240 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =   42339 /  589824 (  7.18%) | total_pruned =  547485 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =      16 /     256 (  6.25%) | total_pruned =     240 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =   42341 /  589824 (  7.18%) | total_pruned =  547483 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =      16 /     256 (  6.25%) | total_pruned =     240 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =   84685 / 1179648 (  7.18%) | total_pruned = 1094963 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =      33 /     512 (  6.45%) | total_pruned =     479 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros =  169371 / 2359296 (  7.18%) | total_pruned = 2189925 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =      33 /     512 (  6.45%) | total_pruned =     479 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =    9406 /  131072 (  7.18%) | total_pruned =  121666 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =      33 /     512 (  6.45%) | total_pruned =     479 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =  169372 / 2359296 (  7.18%) | total_pruned = 2189924 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =      33 /     512 (  6.45%) | total_pruned =     479 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =  169372 / 2359296 (  7.18%) | total_pruned = 2189924 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =      33 /     512 (  6.45%) | total_pruned =     479 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =      33 /     512 (  6.45%) | total_pruned =     479 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 801929, pruned : 10375096, total: 11177025, Compression rate :      13.94x  ( 92.83% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9  31.4\n",
      "  28.2  25.4  22.9  20.6  18.5  16.7  15.   13.5  12.2  10.9   9.8   8.9\n",
      "   8.    7.2   0.    0.    0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "26\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:26/35]: ---\n",
      "model.conv1.weight   | nonzeros =     605 /    9408 (  6.43%) | total_pruned =    8803 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =    2379 /   36864 (  6.45%) | total_pruned =   34485 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =    2379 /   36864 (  6.45%) | total_pruned =   34485 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =    2379 /   36864 (  6.45%) | total_pruned =   34485 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =    2379 /   36864 (  6.45%) | total_pruned =   34485 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =    4761 /   73728 (  6.46%) | total_pruned =   68967 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =       5 /     128 (  3.91%) | total_pruned =     123 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =    9524 /  147456 (  6.46%) | total_pruned =  137932 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =       5 /     128 (  3.91%) | total_pruned =     123 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =     525 /    8192 (  6.41%) | total_pruned =    7667 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =       5 /     128 (  3.91%) | total_pruned =     123 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =    9524 /  147456 (  6.46%) | total_pruned =  137932 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =       5 /     128 (  3.91%) | total_pruned =     123 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =    9524 /  147456 (  6.46%) | total_pruned =  137932 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =       5 /     128 (  3.91%) | total_pruned =     123 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =   19052 /  294912 (  6.46%) | total_pruned =  275860 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =      14 /     256 (  5.47%) | total_pruned =     242 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =   38105 /  589824 (  6.46%) | total_pruned =  551719 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =      14 /     256 (  5.47%) | total_pruned =     242 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =    2114 /   32768 (  6.45%) | total_pruned =   30654 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =      14 /     256 (  5.47%) | total_pruned =     242 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =   38105 /  589824 (  6.46%) | total_pruned =  551719 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =      14 /     256 (  5.47%) | total_pruned =     242 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =   38107 /  589824 (  6.46%) | total_pruned =  551717 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =      14 /     256 (  5.47%) | total_pruned =     242 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =   76216 / 1179648 (  6.46%) | total_pruned = 1103432 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =      29 /     512 (  5.66%) | total_pruned =     483 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros =  152434 / 2359296 (  6.46%) | total_pruned = 2206862 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =      29 /     512 (  5.66%) | total_pruned =     483 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =    8465 /  131072 (  6.46%) | total_pruned =  122607 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =      29 /     512 (  5.66%) | total_pruned =     483 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =  152434 / 2359296 (  6.46%) | total_pruned = 2206862 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =      29 /     512 (  5.66%) | total_pruned =     483 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =  152435 / 2359296 (  6.46%) | total_pruned = 2206861 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =      29 /     512 (  5.66%) | total_pruned =     483 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =      29 /     512 (  5.66%) | total_pruned =     483 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 721721, pruned : 10455304, total: 11177025, Compression rate :      15.49x  ( 93.54% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9  31.4\n",
      "  28.2  25.4  22.9  20.6  18.5  16.7  15.   13.5  12.2  10.9   9.8   8.9\n",
      "   8.    7.2   6.5   0.    0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "27\n",
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:27/35]: ---\n",
      "model.conv1.weight   | nonzeros =     544 /    9408 (  5.78%) | total_pruned =    8864 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =    2141 /   36864 (  5.81%) | total_pruned =   34723 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =    2141 /   36864 (  5.81%) | total_pruned =   34723 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =    2141 /   36864 (  5.81%) | total_pruned =   34723 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =    2141 /   36864 (  5.81%) | total_pruned =   34723 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =    4285 /   73728 (  5.81%) | total_pruned =   69443 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =       4 /     128 (  3.12%) | total_pruned =     124 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =    8571 /  147456 (  5.81%) | total_pruned =  138885 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =       4 /     128 (  3.12%) | total_pruned =     124 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =     472 /    8192 (  5.76%) | total_pruned =    7720 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =       4 /     128 (  3.12%) | total_pruned =     124 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layer2.1.conv1.weight | nonzeros =    8571 /  147456 (  5.81%) | total_pruned =  138885 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =       4 /     128 (  3.12%) | total_pruned =     124 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =    8571 /  147456 (  5.81%) | total_pruned =  138885 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =       4 /     128 (  3.12%) | total_pruned =     124 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =   17146 /  294912 (  5.81%) | total_pruned =  277766 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =      12 /     256 (  4.69%) | total_pruned =     244 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =   34294 /  589824 (  5.81%) | total_pruned =  555530 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =      12 /     256 (  4.69%) | total_pruned =     244 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =    1902 /   32768 (  5.80%) | total_pruned =   30866 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =      12 /     256 (  4.69%) | total_pruned =     244 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =   34294 /  589824 (  5.81%) | total_pruned =  555530 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =      12 /     256 (  4.69%) | total_pruned =     244 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =   34296 /  589824 (  5.81%) | total_pruned =  555528 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =      12 /     256 (  4.69%) | total_pruned =     244 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =   68594 / 1179648 (  5.81%) | total_pruned = 1111054 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =      26 /     512 (  5.08%) | total_pruned =     486 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros =  137190 / 2359296 (  5.81%) | total_pruned = 2222106 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =      26 /     512 (  5.08%) | total_pruned =     486 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =    7618 /  131072 (  5.81%) | total_pruned =  123454 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =      26 /     512 (  5.08%) | total_pruned =     486 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =  137190 / 2359296 (  5.81%) | total_pruned = 2222106 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =      26 /     512 (  5.08%) | total_pruned =     486 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =  137191 / 2359296 (  5.81%) | total_pruned = 2222105 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =      26 /     512 (  5.08%) | total_pruned =     486 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =      26 /     512 (  5.08%) | total_pruned =     486 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 649535, pruned : 10527490, total: 11177025, Compression rate :      17.21x  ( 94.19% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9  31.4\n",
      "  28.2  25.4  22.9  20.6  18.5  16.7  15.   13.5  12.2  10.9   9.8   8.9\n",
      "   8.    7.2   6.5   5.8   0.    0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "28\n",
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:28/35]: ---\n",
      "model.conv1.weight   | nonzeros =     489 /    9408 (  5.20%) | total_pruned =    8919 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =    1927 /   36864 (  5.23%) | total_pruned =   34937 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =    1927 /   36864 (  5.23%) | total_pruned =   34937 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =    1927 /   36864 (  5.23%) | total_pruned =   34937 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =    1927 /   36864 (  5.23%) | total_pruned =   34937 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =    3856 /   73728 (  5.23%) | total_pruned =   69872 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =       3 /     128 (  2.34%) | total_pruned =     125 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =    7714 /  147456 (  5.23%) | total_pruned =  139742 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =       3 /     128 (  2.34%) | total_pruned =     125 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =     424 /    8192 (  5.18%) | total_pruned =    7768 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =       3 /     128 (  2.34%) | total_pruned =     125 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =    7714 /  147456 (  5.23%) | total_pruned =  139742 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =       3 /     128 (  2.34%) | total_pruned =     125 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =    7714 /  147456 (  5.23%) | total_pruned =  139742 | shape = (128, 128, 3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layer2.1.bn2.weight | nonzeros =       3 /     128 (  2.34%) | total_pruned =     125 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =   15431 /  294912 (  5.23%) | total_pruned =  279481 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =      10 /     256 (  3.91%) | total_pruned =     246 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =   30864 /  589824 (  5.23%) | total_pruned =  558960 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =      10 /     256 (  3.91%) | total_pruned =     246 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =    1711 /   32768 (  5.22%) | total_pruned =   31057 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =      10 /     256 (  3.91%) | total_pruned =     246 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =   30864 /  589824 (  5.23%) | total_pruned =  558960 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =      10 /     256 (  3.91%) | total_pruned =     246 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =   30866 /  589824 (  5.23%) | total_pruned =  558958 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =      10 /     256 (  3.91%) | total_pruned =     246 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =   61734 / 1179648 (  5.23%) | total_pruned = 1117914 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =      23 /     512 (  4.49%) | total_pruned =     489 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros =  123471 / 2359296 (  5.23%) | total_pruned = 2235825 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =      23 /     512 (  4.49%) | total_pruned =     489 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =    6856 /  131072 (  5.23%) | total_pruned =  124216 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =      23 /     512 (  4.49%) | total_pruned =     489 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =  123471 / 2359296 (  5.23%) | total_pruned = 2235825 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =      23 /     512 (  4.49%) | total_pruned =     489 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =  123472 / 2359296 (  5.23%) | total_pruned = 2235824 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =      23 /     512 (  4.49%) | total_pruned =     489 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =      23 /     512 (  4.49%) | total_pruned =     489 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 584568, pruned : 10592457, total: 11177025, Compression rate :      19.12x  ( 94.77% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9  31.4\n",
      "  28.2  25.4  22.9  20.6  18.5  16.7  15.   13.5  12.2  10.9   9.8   8.9\n",
      "   8.    7.2   6.5   5.8   5.2   0.    0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "29\n",
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:29/35]: ---\n",
      "model.conv1.weight   | nonzeros =     440 /    9408 (  4.68%) | total_pruned =    8968 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =    1734 /   36864 (  4.70%) | total_pruned =   35130 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =    1734 /   36864 (  4.70%) | total_pruned =   35130 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =    1734 /   36864 (  4.70%) | total_pruned =   35130 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =    1734 /   36864 (  4.70%) | total_pruned =   35130 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =    3470 /   73728 (  4.71%) | total_pruned =   70258 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =       2 /     128 (  1.56%) | total_pruned =     126 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =    6942 /  147456 (  4.71%) | total_pruned =  140514 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =       2 /     128 (  1.56%) | total_pruned =     126 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =     381 /    8192 (  4.65%) | total_pruned =    7811 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =       2 /     128 (  1.56%) | total_pruned =     126 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =    6942 /  147456 (  4.71%) | total_pruned =  140514 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =       2 /     128 (  1.56%) | total_pruned =     126 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =    6942 /  147456 (  4.71%) | total_pruned =  140514 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =       2 /     128 (  1.56%) | total_pruned =     126 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =   13888 /  294912 (  4.71%) | total_pruned =  281024 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =       9 /     256 (  3.52%) | total_pruned =     247 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layer3.0.conv2.weight | nonzeros =   27777 /  589824 (  4.71%) | total_pruned =  562047 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =       9 /     256 (  3.52%) | total_pruned =     247 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =    1540 /   32768 (  4.70%) | total_pruned =   31228 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =       9 /     256 (  3.52%) | total_pruned =     247 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =   27777 /  589824 (  4.71%) | total_pruned =  562047 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =       9 /     256 (  3.52%) | total_pruned =     247 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =   27779 /  589824 (  4.71%) | total_pruned =  562045 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =       9 /     256 (  3.52%) | total_pruned =     247 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =   55560 / 1179648 (  4.71%) | total_pruned = 1124088 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =      20 /     512 (  3.91%) | total_pruned =     492 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros =  111124 / 2359296 (  4.71%) | total_pruned = 2248172 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =      20 /     512 (  3.91%) | total_pruned =     492 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =    6170 /  131072 (  4.71%) | total_pruned =  124902 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =      20 /     512 (  3.91%) | total_pruned =     492 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =  111124 / 2359296 (  4.71%) | total_pruned = 2248172 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =      20 /     512 (  3.91%) | total_pruned =     492 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =  111124 / 2359296 (  4.71%) | total_pruned = 2248172 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =      20 /     512 (  3.91%) | total_pruned =     492 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =      20 /     512 (  3.91%) | total_pruned =     492 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 526097, pruned : 10650928, total: 11177025, Compression rate :      21.25x  ( 95.29% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9  31.4\n",
      "  28.2  25.4  22.9  20.6  18.5  16.7  15.   13.5  12.2  10.9   9.8   8.9\n",
      "   8.    7.2   6.5   5.8   5.2   4.7   0.    0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "30\n",
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:30/35]: ---\n",
      "model.conv1.weight   | nonzeros =     396 /    9408 (  4.21%) | total_pruned =    9012 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =    1560 /   36864 (  4.23%) | total_pruned =   35304 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =    1560 /   36864 (  4.23%) | total_pruned =   35304 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =    1560 /   36864 (  4.23%) | total_pruned =   35304 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =    1560 /   36864 (  4.23%) | total_pruned =   35304 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =    3123 /   73728 (  4.24%) | total_pruned =   70605 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =    6247 /  147456 (  4.24%) | total_pruned =  141209 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =     343 /    8192 (  4.19%) | total_pruned =    7849 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =    6247 /  147456 (  4.24%) | total_pruned =  141209 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =    6247 /  147456 (  4.24%) | total_pruned =  141209 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =   12499 /  294912 (  4.24%) | total_pruned =  282413 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =       8 /     256 (  3.12%) | total_pruned =     248 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =   24999 /  589824 (  4.24%) | total_pruned =  564825 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =       8 /     256 (  3.12%) | total_pruned =     248 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =    1386 /   32768 (  4.23%) | total_pruned =   31382 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =       8 /     256 (  3.12%) | total_pruned =     248 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layer3.1.conv1.weight | nonzeros =   24999 /  589824 (  4.24%) | total_pruned =  564825 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =       8 /     256 (  3.12%) | total_pruned =     248 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =   25001 /  589824 (  4.24%) | total_pruned =  564823 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =       8 /     256 (  3.12%) | total_pruned =     248 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =   50004 / 1179648 (  4.24%) | total_pruned = 1129644 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =      18 /     512 (  3.52%) | total_pruned =     494 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros =  100011 / 2359296 (  4.24%) | total_pruned = 2259285 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =      18 /     512 (  3.52%) | total_pruned =     494 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =    5553 /  131072 (  4.24%) | total_pruned =  125519 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =      18 /     512 (  3.52%) | total_pruned =     494 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =  100011 / 2359296 (  4.24%) | total_pruned = 2259285 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =      18 /     512 (  3.52%) | total_pruned =     494 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =  100011 / 2359296 (  4.24%) | total_pruned = 2259285 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =      18 /     512 (  3.52%) | total_pruned =     494 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =      18 /     512 (  3.52%) | total_pruned =     494 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 473476, pruned : 10703549, total: 11177025, Compression rate :      23.61x  ( 95.76% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9  31.4\n",
      "  28.2  25.4  22.9  20.6  18.5  16.7  15.   13.5  12.2  10.9   9.8   8.9\n",
      "   8.    7.2   6.5   5.8   5.2   4.7   4.2   0.    0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "31\n",
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:31/35]: ---\n",
      "model.conv1.weight   | nonzeros =     356 /    9408 (  3.78%) | total_pruned =    9052 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =    1404 /   36864 (  3.81%) | total_pruned =   35460 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =    1404 /   36864 (  3.81%) | total_pruned =   35460 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =    1404 /   36864 (  3.81%) | total_pruned =   35460 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =    1404 /   36864 (  3.81%) | total_pruned =   35460 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =    2810 /   73728 (  3.81%) | total_pruned =   70918 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =    5622 /  147456 (  3.81%) | total_pruned =  141834 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =     308 /    8192 (  3.76%) | total_pruned =    7884 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =    5622 /  147456 (  3.81%) | total_pruned =  141834 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =    5622 /  147456 (  3.81%) | total_pruned =  141834 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =   11249 /  294912 (  3.81%) | total_pruned =  283663 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =       7 /     256 (  2.73%) | total_pruned =     249 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =   22499 /  589824 (  3.81%) | total_pruned =  567325 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =       7 /     256 (  2.73%) | total_pruned =     249 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =    1247 /   32768 (  3.81%) | total_pruned =   31521 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =       7 /     256 (  2.73%) | total_pruned =     249 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =   22499 /  589824 (  3.81%) | total_pruned =  567325 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =       7 /     256 (  2.73%) | total_pruned =     249 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =   22501 /  589824 (  3.81%) | total_pruned =  567323 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =       7 /     256 (  2.73%) | total_pruned =     249 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =   45003 / 1179648 (  3.81%) | total_pruned = 1134645 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =      16 /     512 (  3.12%) | total_pruned =     496 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layer4.0.conv2.weight | nonzeros =   90010 / 2359296 (  3.82%) | total_pruned = 2269286 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =      16 /     512 (  3.12%) | total_pruned =     496 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =    4997 /  131072 (  3.81%) | total_pruned =  126075 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =      16 /     512 (  3.12%) | total_pruned =     496 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =   90010 / 2359296 (  3.82%) | total_pruned = 2269286 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =      16 /     512 (  3.12%) | total_pruned =     496 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =   90010 / 2359296 (  3.82%) | total_pruned = 2269286 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =      16 /     512 (  3.12%) | total_pruned =     496 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =      16 /     512 (  3.12%) | total_pruned =     496 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 426123, pruned : 10750902, total: 11177025, Compression rate :      26.23x  ( 96.19% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9  31.4\n",
      "  28.2  25.4  22.9  20.6  18.5  16.7  15.   13.5  12.2  10.9   9.8   8.9\n",
      "   8.    7.2   6.5   5.8   5.2   4.7   4.2   3.8   0.    0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "32\n",
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:32/35]: ---\n",
      "model.conv1.weight   | nonzeros =     320 /    9408 (  3.40%) | total_pruned =    9088 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =    1263 /   36864 (  3.43%) | total_pruned =   35601 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =    1263 /   36864 (  3.43%) | total_pruned =   35601 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =    1263 /   36864 (  3.43%) | total_pruned =   35601 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =    1263 /   36864 (  3.43%) | total_pruned =   35601 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =    2529 /   73728 (  3.43%) | total_pruned =   71199 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =    5059 /  147456 (  3.43%) | total_pruned =  142397 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =     277 /    8192 (  3.38%) | total_pruned =    7915 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =    5059 /  147456 (  3.43%) | total_pruned =  142397 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =    5059 /  147456 (  3.43%) | total_pruned =  142397 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =   10124 /  294912 (  3.43%) | total_pruned =  284788 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =       6 /     256 (  2.34%) | total_pruned =     250 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =   20249 /  589824 (  3.43%) | total_pruned =  569575 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =       6 /     256 (  2.34%) | total_pruned =     250 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =    1122 /   32768 (  3.42%) | total_pruned =   31646 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =       6 /     256 (  2.34%) | total_pruned =     250 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =   20249 /  589824 (  3.43%) | total_pruned =  569575 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =       6 /     256 (  2.34%) | total_pruned =     250 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =   20251 /  589824 (  3.43%) | total_pruned =  569573 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =       6 /     256 (  2.34%) | total_pruned =     250 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =   40502 / 1179648 (  3.43%) | total_pruned = 1139146 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =      14 /     512 (  2.73%) | total_pruned =     498 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layer4.0.conv2.weight | nonzeros =   81009 / 2359296 (  3.43%) | total_pruned = 2278287 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =      14 /     512 (  2.73%) | total_pruned =     498 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =    4497 /  131072 (  3.43%) | total_pruned =  126575 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =      14 /     512 (  2.73%) | total_pruned =     498 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =   81009 / 2359296 (  3.43%) | total_pruned = 2278287 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =      14 /     512 (  2.73%) | total_pruned =     498 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =   81009 / 2359296 (  3.43%) | total_pruned = 2278287 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =      14 /     512 (  2.73%) | total_pruned =     498 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =      14 /     512 (  2.73%) | total_pruned =     498 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 383501, pruned : 10793524, total: 11177025, Compression rate :      29.14x  ( 96.57% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9  31.4\n",
      "  28.2  25.4  22.9  20.6  18.5  16.7  15.   13.5  12.2  10.9   9.8   8.9\n",
      "   8.    7.2   6.5   5.8   5.2   4.7   4.2   3.8   3.4   0.    0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "33\n",
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:33/35]: ---\n",
      "model.conv1.weight   | nonzeros =     288 /    9408 (  3.06%) | total_pruned =    9120 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =    1136 /   36864 (  3.08%) | total_pruned =   35728 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =    1136 /   36864 (  3.08%) | total_pruned =   35728 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =    1136 /   36864 (  3.08%) | total_pruned =   35728 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =    1136 /   36864 (  3.08%) | total_pruned =   35728 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =    2276 /   73728 (  3.09%) | total_pruned =   71452 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =    4553 /  147456 (  3.09%) | total_pruned =  142903 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =     249 /    8192 (  3.04%) | total_pruned =    7943 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =    4553 /  147456 (  3.09%) | total_pruned =  142903 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =    4553 /  147456 (  3.09%) | total_pruned =  142903 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =    9111 /  294912 (  3.09%) | total_pruned =  285801 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =       5 /     256 (  1.95%) | total_pruned =     251 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =   18224 /  589824 (  3.09%) | total_pruned =  571600 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =       5 /     256 (  1.95%) | total_pruned =     251 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =    1009 /   32768 (  3.08%) | total_pruned =   31759 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =       5 /     256 (  1.95%) | total_pruned =     251 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =   18224 /  589824 (  3.09%) | total_pruned =  571600 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =       5 /     256 (  1.95%) | total_pruned =     251 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =   18226 /  589824 (  3.09%) | total_pruned =  571598 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =       5 /     256 (  1.95%) | total_pruned =     251 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =   36451 / 1179648 (  3.09%) | total_pruned = 1143197 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =      12 /     512 (  2.34%) | total_pruned =     500 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layer4.0.conv2.weight | nonzeros =   72908 / 2359296 (  3.09%) | total_pruned = 2286388 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =      12 /     512 (  2.34%) | total_pruned =     500 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =    4047 /  131072 (  3.09%) | total_pruned =  127025 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =      12 /     512 (  2.34%) | total_pruned =     500 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros =   72908 / 2359296 (  3.09%) | total_pruned = 2286388 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =      12 /     512 (  2.34%) | total_pruned =     500 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =   72908 / 2359296 (  3.09%) | total_pruned = 2286388 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =      12 /     512 (  2.34%) | total_pruned =     500 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =      12 /     512 (  2.34%) | total_pruned =     500 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 345140, pruned : 10831885, total: 11177025, Compression rate :      32.38x  ( 96.91% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9  31.4\n",
      "  28.2  25.4  22.9  20.6  18.5  16.7  15.   13.5  12.2  10.9   9.8   8.9\n",
      "   8.    7.2   6.5   5.8   5.2   4.7   4.2   3.8   3.4   3.1   0. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "34\n",
      "------\n",
      "#####\n",
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n",
      "\n",
      "--- Pruning Level [1:34/35]: ---\n",
      "model.conv1.weight   | nonzeros =     259 /    9408 (  2.75%) | total_pruned =    9149 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =    1022 /   36864 (  2.77%) | total_pruned =   35842 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =    1022 /   36864 (  2.77%) | total_pruned =   35842 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =    1022 /   36864 (  2.77%) | total_pruned =   35842 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =    1022 /   36864 (  2.77%) | total_pruned =   35842 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =       1 /      64 (  1.56%) | total_pruned =      63 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =    2048 /   73728 (  2.78%) | total_pruned =   71680 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =    4097 /  147456 (  2.78%) | total_pruned =  143359 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =     224 /    8192 (  2.73%) | total_pruned =    7968 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =    4097 /  147456 (  2.78%) | total_pruned =  143359 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =    4097 /  147456 (  2.78%) | total_pruned =  143359 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =       1 /     128 (  0.78%) | total_pruned =     127 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =    8200 /  294912 (  2.78%) | total_pruned =  286712 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =       4 /     256 (  1.56%) | total_pruned =     252 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =   16401 /  589824 (  2.78%) | total_pruned =  573423 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =       4 /     256 (  1.56%) | total_pruned =     252 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =     908 /   32768 (  2.77%) | total_pruned =   31860 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =       4 /     256 (  1.56%) | total_pruned =     252 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =   16401 /  589824 (  2.78%) | total_pruned =  573423 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =       4 /     256 (  1.56%) | total_pruned =     252 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =   16403 /  589824 (  2.78%) | total_pruned =  573421 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =       4 /     256 (  1.56%) | total_pruned =     252 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros =   32806 / 1179648 (  2.78%) | total_pruned = 1146842 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =      10 /     512 (  1.95%) | total_pruned =     502 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros =   65617 / 2359296 (  2.78%) | total_pruned = 2293679 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =      10 /     512 (  1.95%) | total_pruned =     502 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =    3642 /  131072 (  2.78%) | total_pruned =  127430 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =      10 /     512 (  1.95%) | total_pruned =     502 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layer4.1.conv1.weight | nonzeros =   65617 / 2359296 (  2.78%) | total_pruned = 2293679 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =      10 /     512 (  1.95%) | total_pruned =     502 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros =   65617 / 2359296 (  2.78%) | total_pruned = 2293679 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =      10 /     512 (  1.95%) | total_pruned =     502 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =      10 /     512 (  1.95%) | total_pruned =     502 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 310613, pruned : 10866412, total: 11177025, Compression rate :      35.98x  ( 97.22% pruned)\n",
      "[100.   90.   81.   72.9  65.6  59.   53.1  47.8  43.   38.7  34.9  31.4\n",
      "  28.2  25.4  22.9  20.6  18.5  16.7  15.   13.5  12.2  10.9   9.8   8.9\n",
      "   8.    7.2   6.5   5.8   5.2   4.7   4.2   3.8   3.4   3.1   2.8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0000, -0.0000, -0.0000, -0.0000, -0.0577, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0694]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_3_7",
   "language": "python",
   "name": "python_3_7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
