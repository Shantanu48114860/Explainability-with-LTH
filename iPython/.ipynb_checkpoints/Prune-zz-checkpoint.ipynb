{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "import torchvision.utils as vutils\n",
    "import torch.nn.init as init\n",
    "import pickle\n",
    "\n",
    "sys.path.append(os.path.abspath(\"/ocean/projects/asc170022p/shg121/PhD/Project_Pruning\"))\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model_factory.model_meta import Model_Meta\n",
    "from model_factory.models import Classifier\n",
    "from run_manager import RunManager\n",
    "\n",
    "\n",
    "import utils\n",
    "import yaml\n",
    "import pickle\n",
    "import torch\n",
    "from dataset.dataset_mnist import Dataset_mnist\n",
    "from dataset.dataset_utils import get_dataset, get_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "device = utils.get_device()\n",
    "print(f\"Device: {device}\")\n",
    "data_root = \"/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/data/MNIST_EVEN_ODD\"\n",
    "json_root = \"/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/scripts_data\"\n",
    "model_arch = \"Resnet_18\"\n",
    "dataset_name = \"mnist\"\n",
    "pretrained = True\n",
    "transfer_learning = False\n",
    "chk_pt_path = \"seq_epoch_20.pth.tar\"\n",
    "num_classes = 1\n",
    "logs = \"/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/output\"\n",
    "bb_layer = \"layer3\"  # layer3\n",
    "concept_names = [\"Zero\", \"One\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\", \"Nine\"]\n",
    "img_size = 224\n",
    "batch_size = 3\n",
    "epochs = 50\n",
    "num_workers = 4\n",
    "class_list = [0, 1]\n",
    "num_labels = len(class_list)\n",
    "cav_vector_file = \"max_pooled_train_cavs.pkl\"\n",
    "kernel_size={\n",
    "    \"layer3\": 14,\n",
    "    \"layer4\": 7\n",
    "}\n",
    "\n",
    "prune_type = \"lt\"\n",
    "lr = 1e-3\n",
    "ITERATION  = 35\n",
    "prune_percent = 10\n",
    "start_iter = 0\n",
    "end_iter = 100\n",
    "resample = False\n",
    "reinit = True if prune_type==\"reinit\" else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the [train] dataset: 48000\n",
      "Length of the [val] dataset: 12000\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "train_set = get_dataset(\n",
    "    data_root=data_root,\n",
    "    json_root=json_root,\n",
    "    dataset_name=dataset_name,\n",
    "    mode=\"train\"\n",
    ")\n",
    "\n",
    "val_set = get_dataset(\n",
    "    data_root=data_root,\n",
    "    json_root=json_root,\n",
    "    dataset_name=dataset_name,\n",
    "    mode=\"val\"\n",
    ")\n",
    "\n",
    "transform = get_transforms(size=img_size)\n",
    "train_dataset = Dataset_mnist(train_set, transform)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    num_workers=4,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataset = Dataset_mnist(val_set, transform)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    num_workers=4,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    '''\n",
    "    Usage:\n",
    "        model = Model()\n",
    "        model.apply(weight_init)\n",
    "    '''\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        init.normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.Conv3d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose1d):\n",
    "        init.normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose2d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose3d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm3d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.LSTMCell):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.GRU):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.GRUCell):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global model\n",
    "model = Classifier(model_arch, num_classes, pretrained, transfer_learning)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight torch.Size([64, 3, 7, 7])\n",
      "model.bn1.weight torch.Size([64])\n",
      "model.bn1.bias torch.Size([64])\n",
      "model.layer1.0.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "model.layer1.0.bn1.weight torch.Size([64])\n",
      "model.layer1.0.bn1.bias torch.Size([64])\n",
      "model.layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "model.layer1.0.bn2.weight torch.Size([64])\n",
      "model.layer1.0.bn2.bias torch.Size([64])\n",
      "model.layer1.1.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "model.layer1.1.bn1.weight torch.Size([64])\n",
      "model.layer1.1.bn1.bias torch.Size([64])\n",
      "model.layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "model.layer1.1.bn2.weight torch.Size([64])\n",
      "model.layer1.1.bn2.bias torch.Size([64])\n",
      "model.layer2.0.conv1.weight torch.Size([128, 64, 3, 3])\n",
      "model.layer2.0.bn1.weight torch.Size([128])\n",
      "model.layer2.0.bn1.bias torch.Size([128])\n",
      "model.layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "model.layer2.0.bn2.weight torch.Size([128])\n",
      "model.layer2.0.bn2.bias torch.Size([128])\n",
      "model.layer2.0.downsample.0.weight torch.Size([128, 64, 1, 1])\n",
      "model.layer2.0.downsample.1.weight torch.Size([128])\n",
      "model.layer2.0.downsample.1.bias torch.Size([128])\n",
      "model.layer2.1.conv1.weight torch.Size([128, 128, 3, 3])\n",
      "model.layer2.1.bn1.weight torch.Size([128])\n",
      "model.layer2.1.bn1.bias torch.Size([128])\n",
      "model.layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "model.layer2.1.bn2.weight torch.Size([128])\n",
      "model.layer2.1.bn2.bias torch.Size([128])\n",
      "model.layer3.0.conv1.weight torch.Size([256, 128, 3, 3])\n",
      "model.layer3.0.bn1.weight torch.Size([256])\n",
      "model.layer3.0.bn1.bias torch.Size([256])\n",
      "model.layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "model.layer3.0.bn2.weight torch.Size([256])\n",
      "model.layer3.0.bn2.bias torch.Size([256])\n",
      "model.layer3.0.downsample.0.weight torch.Size([256, 128, 1, 1])\n",
      "model.layer3.0.downsample.1.weight torch.Size([256])\n",
      "model.layer3.0.downsample.1.bias torch.Size([256])\n",
      "model.layer3.1.conv1.weight torch.Size([256, 256, 3, 3])\n",
      "model.layer3.1.bn1.weight torch.Size([256])\n",
      "model.layer3.1.bn1.bias torch.Size([256])\n",
      "model.layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "model.layer3.1.bn2.weight torch.Size([256])\n",
      "model.layer3.1.bn2.bias torch.Size([256])\n",
      "model.layer4.0.conv1.weight torch.Size([512, 256, 3, 3])\n",
      "model.layer4.0.bn1.weight torch.Size([512])\n",
      "model.layer4.0.bn1.bias torch.Size([512])\n",
      "model.layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "model.layer4.0.bn2.weight torch.Size([512])\n",
      "model.layer4.0.bn2.bias torch.Size([512])\n",
      "model.layer4.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "model.layer4.0.downsample.1.weight torch.Size([512])\n",
      "model.layer4.0.downsample.1.bias torch.Size([512])\n",
      "model.layer4.1.conv1.weight torch.Size([512, 512, 3, 3])\n",
      "model.layer4.1.bn1.weight torch.Size([512])\n",
      "model.layer4.1.bn1.bias torch.Size([512])\n",
      "model.layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "model.layer4.1.bn2.weight torch.Size([512])\n",
      "model.layer4.1.bn2.bias torch.Size([512])\n",
      "model.fc.0.weight torch.Size([1, 512])\n",
      "model.fc.0.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[-0.0104, -0.0061, -0.0018,  0.0748,  0.0566,  0.0171, -0.0127],\n",
      "        [ 0.0111,  0.0095, -0.1099, -0.2805, -0.2712, -0.1291,  0.0037],\n",
      "        [-0.0069,  0.0591,  0.2955,  0.5872,  0.5197,  0.2563,  0.0636],\n",
      "        [ 0.0305, -0.0670, -0.2984, -0.4387, -0.2709, -0.0006,  0.0576],\n",
      "        [-0.0275,  0.0160,  0.0726, -0.0541, -0.3328, -0.4206, -0.2578],\n",
      "        [ 0.0306,  0.0410,  0.0628,  0.2390,  0.4138,  0.3936,  0.1661],\n",
      "        [-0.0137, -0.0037, -0.0241, -0.0659, -0.1507, -0.0822, -0.0058]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param[0][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.conv1.weight tensor([[ 0.0299, -0.0026, -0.0051,  0.0486,  0.0018, -0.0319,  0.0104],\n",
      "        [ 0.0123,  0.0202, -0.0153,  0.0333,  0.0261,  0.0053, -0.0280],\n",
      "        [ 0.0233, -0.0267, -0.0044, -0.0001, -0.0066, -0.0065, -0.0295],\n",
      "        [-0.0304,  0.0186, -0.0403,  0.0035, -0.0094, -0.0013,  0.0125],\n",
      "        [-0.0188,  0.0122, -0.0129, -0.0221, -0.0008, -0.0063,  0.0327],\n",
      "        [-0.0093,  0.0137,  0.0181,  0.0178, -0.0103,  0.0218, -0.0203],\n",
      "        [-0.0383,  0.0038,  0.0386, -0.0114,  0.0342,  0.0154,  0.0481]],\n",
      "       device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "model.apply(weight_init)\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param[0][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mask(model):\n",
    "    global step\n",
    "    global mask\n",
    "    step = 0\n",
    "    for name, param in model.named_parameters(): \n",
    "        if 'weight' in name:\n",
    "            step = step + 1\n",
    "    mask = [None]* step \n",
    "    print(mask)\n",
    "    step = 0\n",
    "    for name, param in model.named_parameters(): \n",
    "        if 'weight' in name:\n",
    "            tensor = param.data.cpu().numpy()\n",
    "            mask[step] = np.ones_like(tensor)\n",
    "            step = step + 1\n",
    "    print(step)\n",
    "    step = 0\n",
    "    print(type(mask))\n",
    "    print(len(mask))\n",
    "    print(mask[0][0])\n",
    "    \n",
    "    print(\"Final list: \")\n",
    "    idx = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"name: {name}, param_size: {param.size()}, mask_size: {np.array(mask[idx]).shape}\")\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint directory is created successfully at:\n",
      "/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/output/chk_pt/Pruning/Resnet_18/mnist\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "41\n",
      "<class 'list'>\n",
      "41\n",
      "[[[1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]]]\n",
      "Final list: \n",
      "name: model.conv1.weight, param_size: torch.Size([64, 3, 7, 7]), mask_size: (64, 3, 7, 7)\n",
      "name: model.bn1.weight, param_size: torch.Size([64]), mask_size: (64,)\n",
      "name: model.layer1.0.conv1.weight, param_size: torch.Size([64, 64, 3, 3]), mask_size: (64, 64, 3, 3)\n",
      "name: model.layer1.0.bn1.weight, param_size: torch.Size([64]), mask_size: (64,)\n",
      "name: model.layer1.0.conv2.weight, param_size: torch.Size([64, 64, 3, 3]), mask_size: (64, 64, 3, 3)\n",
      "name: model.layer1.0.bn2.weight, param_size: torch.Size([64]), mask_size: (64,)\n",
      "name: model.layer1.1.conv1.weight, param_size: torch.Size([64, 64, 3, 3]), mask_size: (64, 64, 3, 3)\n",
      "name: model.layer1.1.bn1.weight, param_size: torch.Size([64]), mask_size: (64,)\n",
      "name: model.layer1.1.conv2.weight, param_size: torch.Size([64, 64, 3, 3]), mask_size: (64, 64, 3, 3)\n",
      "name: model.layer1.1.bn2.weight, param_size: torch.Size([64]), mask_size: (64,)\n",
      "name: model.layer2.0.conv1.weight, param_size: torch.Size([128, 64, 3, 3]), mask_size: (128, 64, 3, 3)\n",
      "name: model.layer2.0.bn1.weight, param_size: torch.Size([128]), mask_size: (128,)\n",
      "name: model.layer2.0.conv2.weight, param_size: torch.Size([128, 128, 3, 3]), mask_size: (128, 128, 3, 3)\n",
      "name: model.layer2.0.bn2.weight, param_size: torch.Size([128]), mask_size: (128,)\n",
      "name: model.layer2.0.downsample.0.weight, param_size: torch.Size([128, 64, 1, 1]), mask_size: (128, 64, 1, 1)\n",
      "name: model.layer2.0.downsample.1.weight, param_size: torch.Size([128]), mask_size: (128,)\n",
      "name: model.layer2.1.conv1.weight, param_size: torch.Size([128, 128, 3, 3]), mask_size: (128, 128, 3, 3)\n",
      "name: model.layer2.1.bn1.weight, param_size: torch.Size([128]), mask_size: (128,)\n",
      "name: model.layer2.1.conv2.weight, param_size: torch.Size([128, 128, 3, 3]), mask_size: (128, 128, 3, 3)\n",
      "name: model.layer2.1.bn2.weight, param_size: torch.Size([128]), mask_size: (128,)\n",
      "name: model.layer3.0.conv1.weight, param_size: torch.Size([256, 128, 3, 3]), mask_size: (256, 128, 3, 3)\n",
      "name: model.layer3.0.bn1.weight, param_size: torch.Size([256]), mask_size: (256,)\n",
      "name: model.layer3.0.conv2.weight, param_size: torch.Size([256, 256, 3, 3]), mask_size: (256, 256, 3, 3)\n",
      "name: model.layer3.0.bn2.weight, param_size: torch.Size([256]), mask_size: (256,)\n",
      "name: model.layer3.0.downsample.0.weight, param_size: torch.Size([256, 128, 1, 1]), mask_size: (256, 128, 1, 1)\n",
      "name: model.layer3.0.downsample.1.weight, param_size: torch.Size([256]), mask_size: (256,)\n",
      "name: model.layer3.1.conv1.weight, param_size: torch.Size([256, 256, 3, 3]), mask_size: (256, 256, 3, 3)\n",
      "name: model.layer3.1.bn1.weight, param_size: torch.Size([256]), mask_size: (256,)\n",
      "name: model.layer3.1.conv2.weight, param_size: torch.Size([256, 256, 3, 3]), mask_size: (256, 256, 3, 3)\n",
      "name: model.layer3.1.bn2.weight, param_size: torch.Size([256]), mask_size: (256,)\n",
      "name: model.layer4.0.conv1.weight, param_size: torch.Size([512, 256, 3, 3]), mask_size: (512, 256, 3, 3)\n",
      "name: model.layer4.0.bn1.weight, param_size: torch.Size([512]), mask_size: (512,)\n",
      "name: model.layer4.0.conv2.weight, param_size: torch.Size([512, 512, 3, 3]), mask_size: (512, 512, 3, 3)\n",
      "name: model.layer4.0.bn2.weight, param_size: torch.Size([512]), mask_size: (512,)\n",
      "name: model.layer4.0.downsample.0.weight, param_size: torch.Size([512, 256, 1, 1]), mask_size: (512, 256, 1, 1)\n",
      "name: model.layer4.0.downsample.1.weight, param_size: torch.Size([512]), mask_size: (512,)\n",
      "name: model.layer4.1.conv1.weight, param_size: torch.Size([512, 512, 3, 3]), mask_size: (512, 512, 3, 3)\n",
      "name: model.layer4.1.bn1.weight, param_size: torch.Size([512]), mask_size: (512,)\n",
      "name: model.layer4.1.conv2.weight, param_size: torch.Size([512, 512, 3, 3]), mask_size: (512, 512, 3, 3)\n",
      "name: model.layer4.1.bn2.weight, param_size: torch.Size([512]), mask_size: (512,)\n",
      "name: model.fc.0.weight, param_size: torch.Size([1, 512]), mask_size: (1, 512)\n"
     ]
    }
   ],
   "source": [
    "chk_pt = \"/ocean/projects/asc170022p/shg121/PhD/Project_Pruning/output/chk_pt/Pruning\"\n",
    "chk_pt_file = os.path.join(chk_pt, model_arch, dataset_name)\n",
    "try:\n",
    "    os.makedirs(chk_pt_file, exist_ok=True)\n",
    "    print(\"Checkpoint directory is created successfully at:\")\n",
    "    print(chk_pt_file)\n",
    "except OSError as error:\n",
    "    print(f\"Checkpoint directory {chk_pt_file} can not be created\")\n",
    "    \n",
    "initial_state_dict = copy.deepcopy(model.state_dict())\n",
    "torch.save(model, os.path.join(chk_pt_file, f\"initial_state_dict_prune_type_{prune_type}.pth.tar\"))\n",
    "\n",
    "make_mask(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "    # Loss Function\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35,)\n",
      "(35,)\n",
      "(100,)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "bestacc = 0.0\n",
    "best_accuracy = 0\n",
    "\n",
    "comp = np.zeros(ITERATION,float)\n",
    "bestacc = np.zeros(ITERATION,float)\n",
    "step = 0\n",
    "all_loss = np.zeros(end_iter,float)\n",
    "all_accuracy = np.zeros(end_iter,float)\n",
    "\n",
    "print(comp.shape)\n",
    "print(bestacc.shape)\n",
    "print(all_loss.shape)\n",
    "print(all_accuracy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_by_percentile(percent, resample=False, reinit=False,**kwargs):\n",
    "        global step\n",
    "        global mask\n",
    "        global model\n",
    "\n",
    "        # Calculate percentile value\n",
    "        step = 0\n",
    "        for name, param in model.named_parameters():\n",
    "\n",
    "            # We do not prune bias term\n",
    "            if 'weight' in name:\n",
    "                tensor = param.data.cpu().numpy()\n",
    "                alive = tensor[np.nonzero(tensor)] # flattened array of nonzero values\n",
    "                percentile_value = np.percentile(abs(alive), percent)\n",
    "\n",
    "                # Convert Tensors to numpy and calculate\n",
    "                weight_dev = param.device\n",
    "                new_mask = np.where(abs(tensor) < percentile_value, 0, mask[step])\n",
    "                \n",
    "                # Apply new weight and mask\n",
    "                param.data = torch.from_numpy(tensor * new_mask).to(weight_dev)\n",
    "                mask[step] = new_mask\n",
    "                step += 1\n",
    "        step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_initialization(mask_temp, initial_state_dict):\n",
    "    global step\n",
    "    global mask\n",
    "    step = 0\n",
    "    for name, param in self.model.named_parameters(): \n",
    "        if \"weight\" in name: \n",
    "            weight_dev = param.device\n",
    "            param.data = torch.from_numpy(mask_temp[step] * initial_state_dict[name].cpu().numpy()).to(weight_dev)\n",
    "            step = step + 1\n",
    "        if \"bias\" in name:\n",
    "            param.data = initial_state_dict[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nonzeros(model):\n",
    "    nonzero = total = 0\n",
    "    for name, p in model.named_parameters():\n",
    "        tensor = p.data.cpu().numpy()\n",
    "        nz_count = np.count_nonzero(tensor)\n",
    "        total_params = np.prod(tensor.shape)\n",
    "        nonzero += nz_count\n",
    "        total += total_params\n",
    "        print(f'{name:20} | nonzeros = {nz_count:7} / {total_params:7} ({100 * nz_count / total_params:6.2f}%) | total_pruned = {total_params - nz_count :7} | shape = {tensor.shape}')\n",
    "    print(f'alive: {nonzero}, pruned : {total - nonzero}, total: {total}, Compression rate : {total/nonzero:10.2f}x  ({100 * (total-nonzero) / total:6.2f}% pruned)')\n",
    "    return (round((nonzero/total)*100,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pruning Level [1:0/35]: ---\n",
      "model.conv1.weight   | nonzeros =    9408 /    9408 (100.00%) | total_pruned =       0 | shape = (64, 3, 7, 7)\n",
      "model.bn1.weight     | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "model.bn1.bias       | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv1.weight | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn1.weight | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "model.layer1.0.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.0.conv2.weight | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "model.layer1.0.bn2.weight | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "model.layer1.0.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv1.weight | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn1.weight | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "model.layer1.1.bn1.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer1.1.conv2.weight | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "model.layer1.1.bn2.weight | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "model.layer1.1.bn2.bias | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "model.layer2.0.conv1.weight | nonzeros =   73728 /   73728 (100.00%) | total_pruned =       0 | shape = (128, 64, 3, 3)\n",
      "model.layer2.0.bn1.weight | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "model.layer2.0.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.conv2.weight | nonzeros =  147456 /  147456 (100.00%) | total_pruned =       0 | shape = (128, 128, 3, 3)\n",
      "model.layer2.0.bn2.weight | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "model.layer2.0.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.0.downsample.0.weight | nonzeros =    8192 /    8192 (100.00%) | total_pruned =       0 | shape = (128, 64, 1, 1)\n",
      "model.layer2.0.downsample.1.weight | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "model.layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv1.weight | nonzeros =  147456 /  147456 (100.00%) | total_pruned =       0 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn1.weight | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "model.layer2.1.bn1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer2.1.conv2.weight | nonzeros =  147456 /  147456 (100.00%) | total_pruned =       0 | shape = (128, 128, 3, 3)\n",
      "model.layer2.1.bn2.weight | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "model.layer2.1.bn2.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "model.layer3.0.conv1.weight | nonzeros =  294912 /  294912 (100.00%) | total_pruned =       0 | shape = (256, 128, 3, 3)\n",
      "model.layer3.0.bn1.weight | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "model.layer3.0.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.conv2.weight | nonzeros =  589824 /  589824 (100.00%) | total_pruned =       0 | shape = (256, 256, 3, 3)\n",
      "model.layer3.0.bn2.weight | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "model.layer3.0.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.0.downsample.0.weight | nonzeros =   32768 /   32768 (100.00%) | total_pruned =       0 | shape = (256, 128, 1, 1)\n",
      "model.layer3.0.downsample.1.weight | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "model.layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv1.weight | nonzeros =  589824 /  589824 (100.00%) | total_pruned =       0 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn1.weight | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "model.layer3.1.bn1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer3.1.conv2.weight | nonzeros =  589824 /  589824 (100.00%) | total_pruned =       0 | shape = (256, 256, 3, 3)\n",
      "model.layer3.1.bn2.weight | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "model.layer3.1.bn2.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "model.layer4.0.conv1.weight | nonzeros = 1179648 / 1179648 (100.00%) | total_pruned =       0 | shape = (512, 256, 3, 3)\n",
      "model.layer4.0.bn1.weight | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "model.layer4.0.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.conv2.weight | nonzeros = 2359296 / 2359296 (100.00%) | total_pruned =       0 | shape = (512, 512, 3, 3)\n",
      "model.layer4.0.bn2.weight | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "model.layer4.0.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.0.downsample.0.weight | nonzeros =  131072 /  131072 (100.00%) | total_pruned =       0 | shape = (512, 256, 1, 1)\n",
      "model.layer4.0.downsample.1.weight | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "model.layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv1.weight | nonzeros = 2359295 / 2359296 (100.00%) | total_pruned =       1 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn1.weight | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "model.layer4.1.bn1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.layer4.1.conv2.weight | nonzeros = 2359296 / 2359296 (100.00%) | total_pruned =       0 | shape = (512, 512, 3, 3)\n",
      "model.layer4.1.bn2.weight | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "model.layer4.1.bn2.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "model.fc.0.weight    | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (1, 512)\n",
      "model.fc.0.bias      | nonzeros =       1 /       1 (100.00%) | total_pruned =       0 | shape = (1,)\n",
      "alive: 11172224, pruned : 4801, total: 11177025, Compression rate :       1.00x  (  0.04% pruned)\n",
      "[100.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [02:35<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ITE = 1\n",
    "for _ite in range(start_iter, ITERATION):\n",
    "    if _ite > 0:\n",
    "        print(_ite)\n",
    "        prune_by_percentile(prune_percent, resample=resample, reinit=reinit)\n",
    "        print(\"------\")\n",
    "        \n",
    "        if reinit:\n",
    "            model.apply(weight_init)\n",
    "            step = 0\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    weight_dev = param.device\n",
    "                    param.data = torch.from_numpy(param.data.cpu().numpy() * mask[step]).to(weight_dev)\n",
    "                    step = step + 1\n",
    "            step = 0\n",
    "        else:\n",
    "            original_initialization(mask, initial_state_dict)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "    \n",
    "    print(f\"\\n--- Pruning Level [{ITE}:{_ite}/{ITERATION}]: ---\")\n",
    "    comp1 = print_nonzeros(model)\n",
    "    comp[_ite] = comp1\n",
    "    print(comp)\n",
    "    pbar = tqdm(range(end_iter))\n",
    "    print(pbar)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "end_iter_ = 2\n",
    "for iter_ in range(end_iter_):\n",
    "    print(iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  1.   2.  44.   2.]\n",
      "  [ 11.  14.   1.   2.]\n",
      "  [ 11.  14.   1.   2.]\n",
      "  [ 11.  14.   1.   2.]]\n",
      "\n",
      " [[  1.   2.  44.   2.]\n",
      "  [ 11.  14.   1.   2.]\n",
      "  [101.  14.   1.   2.]\n",
      "  [ 11.  14.   1.   2.]]]\n",
      "(2, 4, 4)\n",
      "[[ 44.]\n",
      " [101.]]\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "def flatten_cnn_activations(activations, kernel_size, stride=1):\n",
    "    max_pool = torch.nn.MaxPool2d(kernel_size, stride=1)\n",
    "    torch_activation = torch.from_numpy(activations)\n",
    "    max_pool_activation = max_pool(torch_activation)\n",
    "    flatten_activations = max_pool_activation.view(\n",
    "        max_pool_activation.size()[0], -1\n",
    "    ).numpy()\n",
    "    \n",
    "    print(flatten_activations)\n",
    "    print(flatten_activations.shape)\n",
    "    \n",
    "activations = np.array([[[1., 2., 44., 2.], [11., 14., 1., 2.], [11., 14., 1., 2.], [11., 14., 1., 2.]], \n",
    "                       [[1., 2., 44., 2.], [11., 14., 1., 2.], [101., 14., 1., 2.], [11., 14., 1., 2.]]])\n",
    "print(activations)\n",
    "print(activations.shape)\n",
    "flatten_cnn_activations(activations, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_3_7",
   "language": "python",
   "name": "python_3_7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
